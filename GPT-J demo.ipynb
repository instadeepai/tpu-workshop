{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be1533a2-039c-4302-85af-2e0b0138674f",
   "metadata": {},
   "source": [
    "# &#x1F916; LLMs on TPUs: An interactive demo using GPT-J"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbdf0b9-752a-4a29-9b41-8eb61e64250c",
   "metadata": {},
   "source": [
    "This demo will walk through how to use JAX and TPUs to run and train large language models (LLMs).  In particular, we consider an open-source model called GPT-J, which was trained using Jax on Google Cloud TPUs - in particular using the [mesh-transformer-jax](https://github.com/kingoflolz/mesh-transformer-jax) repo - by EleutherAI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0bf8d4-c2a6-4f17-b858-55b76c011eaf",
   "metadata": {},
   "source": [
    "### &#x1F440; Contents\n",
    "* &#x1F6A7; [Setup](#setup)\n",
    "    * &#x2622; [Installation and running options](#install-and-opts)\n",
    "    * &#129470; [Device configuration and imports](#device-and-import)\n",
    "* &#x1F4BE; [Model parameters and config](#params-and-config)\n",
    "* &#x2B50; [Inference](#inference)\n",
    "* &#x1F680; [Fine-tuning](#fine-tuning)\n",
    "    * &#x1F42A; [Dataset](#dataset)\n",
    "    * &#x26A1; [Parameter efficient fine-tuning](#param-efficient-fine-tuning)\n",
    "    * &#x1F6A7; [Set up fine-tuning](#set-up-fine-tuning)\n",
    "    * &#x1F4AC; [Performance test: pre-trained](#perf-pretrained)\n",
    "    * &#x1F3CB; [Training loop](#training-loop)\n",
    "    * &#x1F4AC;[ Performance check: fine-tuned](#perf-fine-tuned)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8664b6ee-fd36-4c7e-9711-2ceb10115131",
   "metadata": {
    "tags": []
   },
   "source": [
    "### &#x1F6A7; Setup <a name=\"setup\"></a>\n",
    "\n",
    "First we will just import a few standard packages, check for available devices and define a few util functions. &#x1F600;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24d4310-7e0b-4958-94ee-f2b4e49ea6a3",
   "metadata": {},
   "source": [
    "##### &#x2622; Installation and running options <a name=\"install-and-opts\"></a>\n",
    "\n",
    "If running in Colab, uncomment the below to install the necessary dependencies.  Note that this assumes the ```gptj-demo``` folder is already in the current working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f55bb67-92a6-4de5-bdad-82ef31bc85f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q transformers==4.25.1 tqdm datasets==2.10.1 dm-haiku==0.0.9\n",
    "# !pip install -e gptj-demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "533c0ecc-f1d5-4349-91b5-419802439360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In low memory demo mode, a tiny model with random parameters will be used.\n",
    "LOW_MEMORY_DEMO_MODE = False\n",
    "if LOW_MEMORY_DEMO_MODE:\n",
    "    print(f\"\\033[93m WARNING: Operating in low memory demo mode will mean a tiny, random, model is used!\\033[0m\")\n",
    "\n",
    "# If TPUs are not available, multiple CPU devices will be faked.\n",
    "TPUS_AVAILABLE = True\n",
    "\n",
    "# Optionally, we can save the converted JAX parameters to disk for re-loading later.  This will make\n",
    "# the first time fetching the model weights slower, but every subsequent time faster.\n",
    "STORE_PARAMS_ON_DISK = True\n",
    "PRETRAINED_PARAMS_PATH = \"./pretrained_params.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc54c98-a97b-48d3-8239-e8e1c8acc92f",
   "metadata": {},
   "source": [
    "##### &#129470; Device configuration and imports <a name=\"device-and-import\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "457d5b07-d599-4346-84e4-5e97dacf35d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default devices set to cpu: [CpuDevice(id=0)]\n",
      "\n",
      "8 devices available.\n",
      "\t TPU_0(process=0,(0,0,0,0))\n",
      "\t TPU_1(process=0,(0,0,0,1))\n",
      "\t TPU_2(process=0,(1,0,0,0))\n",
      "\t TPU_3(process=0,(1,0,0,1))\n",
      "\t TPU_4(process=0,(0,1,0,0))\n",
      "\t TPU_5(process=0,(0,1,0,1))\n",
      "\t TPU_6(process=0,(1,1,0,0))\n",
      "\t TPU_7(process=0,(1,1,0,1))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if not TPUS_AVAILABLE:\n",
    "    # If TPU's aren't available, we can still fake 8 devices.  This must be\n",
    "    # done *before* importing JAX.\n",
    "    flags = os.environ.get(\"XLA_FLAGS\", \"\")\n",
    "    os.environ[\"XLA_FLAGS\"] = flags + \" --xla_force_host_platform_device_count=8\"\n",
    "    os.environ[\"JAX_PLATFORM_NAME\"] = \"cpu\"\n",
    "\n",
    "import jax\n",
    "\n",
    "jax.config.update(\"jax_platform_name\", \"cpu\")\n",
    "devices = jax.devices(\"tpu\" if TPUS_AVAILABLE else \"cpu\")\n",
    "\n",
    "print(f\"Default devices set to cpu: {jax.local_devices()}\")\n",
    "print(f\"\\n{len(devices)} devices available.\")\n",
    "for dev in devices:\n",
    "    print(\"\\t\", dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf6bd67b-ed2c-4581-9052-0dcff19a7078",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import haiku as hk\n",
    "\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "import functools\n",
    "from tqdm import tqdm\n",
    "import reprlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a009f6a9-6633-46eb-a08e-f12069f30a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default devices set to cpu: [CpuDevice(id=0)]\n",
      "\n",
      "8 devices available.\n",
      "\t TPU_0(process=0,(0,0,0,0))\n",
      "\t TPU_1(process=0,(0,0,0,1))\n",
      "\t TPU_2(process=0,(1,0,0,0))\n",
      "\t TPU_3(process=0,(1,0,0,1))\n",
      "\t TPU_4(process=0,(0,1,0,0))\n",
      "\t TPU_5(process=0,(0,1,0,1))\n",
      "\t TPU_6(process=0,(1,1,0,0))\n",
      "\t TPU_7(process=0,(1,1,0,1))\n"
     ]
    }
   ],
   "source": [
    "jax.config.update(\"jax_platform_name\", \"cpu\")\n",
    "devices = jax.devices(\"tpu\")\n",
    "\n",
    "print(f\"Default devices set to cpu: {jax.local_devices()}\")\n",
    "print(f\"\\n{len(devices)} devices available.\")\n",
    "for dev in devices:\n",
    "    print(\"\\t\", dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09965400-aa19-470e-92a0-b3e74cd3a814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small util function for stylised printing.\n",
    "from IPython.display import Markdown, display\n",
    "from typing import Any\n",
    "\n",
    "def printmd(string: str, color=None):\n",
    "    colorstr = \"<span style='color:{}'>{}</span>\".format(color, string)\n",
    "    display(Markdown(colorstr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca18e7b-c330-42f4-8057-43f1c20afb6b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### &#x1F4BE; Model parameters and config <a name=\"params-and-config\"></a>\n",
    "\n",
    "First we fetch pre-trained parameters and a Tokenizer for GPT-J from [HuggingFace](https://huggingface.co/docs/transformers/model_doc/gptj).  For this particular checkpoint, the parameters are in PyTorch arrays, but it is straightforward to convert them to a dictionary of appropriately named parameters for our JAX model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e1ca98b-1d4f-4271-9107-4bad6709a103",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, logging\n",
    "\n",
    "from src.pretrained_utils import translate_torch_params\n",
    "from src.model.model import GptConfig, build_gpt_fn\n",
    "from src.utils.decoding import update_tokens_ids_greedy\n",
    "from src.utils.parameters import get_num_parameters, save_params, load_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "110173a0-84e9-405c-8e15-1d270c2df7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Tokenizer from HuggingFace...done.\n",
      "\n",
      "Loading parameters from ./pretrained_params.gz...done.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<span style='color:blue'>Loaded GPT-J parameters and Tokenizer in 83.0 seconds.</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GPT-J has 6.05B parameters.\n",
      "\n",
      "Parameters are provided as a dictionary...\n",
      "\n",
      " {'gpt_j_decode...tn_layer_norm': {'offset': (4096,), 'scale': (4096,)}, 'gpt_j_decode.../~/fc1_linear': {'b': (16384,), 'w': (4096, 16384)}, 'gpt_j_decode.../~/fc2_linear': {'b': (4096,), 'w': (16384, 4096)}, 'gpt_j_decode.../~/key_linear': {'w': (4096, 4096)}, ...}\n"
     ]
    }
   ],
   "source": [
    "t_start = time.time()\n",
    "\n",
    "print(f\"Loading Tokenizer from HuggingFace\", end=\"...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "print(\"done.\\n\")\n",
    "\n",
    "# Define the config we will use for creating GPT-J models.  If we are not loading pretrained parameters,\n",
    "# assume this is for memory reasons and so make a smaller model.\n",
    "config = GptConfig(\n",
    "    vocab_size=50400,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    embed_dim=4096 if not LOW_MEMORY_DEMO_MODE else 128,\n",
    "    ffn_embed_dim=16384 if not LOW_MEMORY_DEMO_MODE else 128,\n",
    "    num_heads=16  if not LOW_MEMORY_DEMO_MODE else 1,\n",
    "    num_layers=28 if not LOW_MEMORY_DEMO_MODE else 2,\n",
    "    rope_dimensions=64,\n",
    "    max_position_embeddings=2048,\n",
    "    add_bias_ffn=True,\n",
    "    ffn_activation_name=\"gelu\",\n",
    "    use_glu_in_ffn=False,\n",
    "    add_bias_lm_head=True,\n",
    "    norm_type=\"layer_norm\",\n",
    "    parallel_attention_ff=True,\n",
    "    use_gradient_checkpointing=False,\n",
    ")\n",
    "\n",
    "if not LOW_MEMORY_DEMO_MODE:\n",
    "    # Fetch parameters from HuggingFace or disk.\n",
    "    if STORE_PARAMS_ON_DISK and os.path.exists(PRETRAINED_PARAMS_PATH):\n",
    "        print(f\"Loading parameters from {PRETRAINED_PARAMS_PATH}\", end=\"...\")\n",
    "        pretrained_parameters = load_params(PRETRAINED_PARAMS_PATH)\n",
    "        print(\"done.\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Loading parameters from HuggingFace\", end=\"...\")\n",
    "        pytorch_params = AutoModelForCausalLM.from_pretrained(\n",
    "            \"EleutherAI/gpt-j-6B\",\n",
    "            revision=\"float16\",\n",
    "            torch_dtype=torch.float16,\n",
    "        ).state_dict()\n",
    "        print(\"done.\")\n",
    "\n",
    "        print(\"Converting parameters PyTorch to JAX\", end=\"...\")\n",
    "        pretrained_parameters = translate_torch_params(pytorch_params, dtype=jnp.bfloat16)\n",
    "        del pytorch_params\n",
    "        print(\"done.\")\n",
    "\n",
    "        if STORE_PARAMS_ON_DISK:\n",
    "            print(f\"Saving parameters to {PRETRAINED_PARAMS_PATH}\", end=\"...\")\n",
    "            save_params(pretrained_parameters, PRETRAINED_PARAMS_PATH)\n",
    "            print(\"done.\")\n",
    "            \n",
    "else:\n",
    "    # Randomly initialise parameters.\n",
    "    gptj_fn = build_gpt_fn(\n",
    "        config=config,\n",
    "        compute_dtype=jnp.float16,\n",
    "        param_dtype=jnp.float16,\n",
    "        output_dtype=jnp.float16,\n",
    "        name=\"gpt_j_decoder\",\n",
    "    )\n",
    "    init_fn = hk.transform(gptj_fn).init\n",
    "    t_start = time.time()\n",
    "    tokens_ids = tokenizer(\"Test\", return_tensors=\"np\")['input_ids']\n",
    "    print(\"Initialising model with random parameters\", end=\"...\")\n",
    "    pretrained_parameters = init_fn(jax.random.PRNGKey(0), tokens_ids[None])\n",
    "    print(f\"done in {time.time()-t_start:.1f} seconds.\")\n",
    "\n",
    "printmd(f\"Loaded GPT-J parameters and Tokenizer in {time.time() - t_start:.1f} seconds.\", color=\"blue\")\n",
    "print(f\"\\nGPT-J has {get_num_parameters(pretrained_parameters)/1e9:.2f}B parameters.\")\n",
    "print(f\"\\nParameters are provided as a dictionary...\\n\\n\", reprlib.repr(jax.tree_map(lambda x: x.shape, pretrained_parameters)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169f92ef-0190-4a0a-81ed-23f9479678a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### &#x2B50; Inference <a name=\"inference\"></a>\n",
    "\n",
    "Now we can use these to run the pre-trained model.  First we build the Haiku model and wrap into an update function uses it to predict the next token in a sequence.  Then we tokenise our prompt, run inference, and decode the resulting tokens back to text to obtain our output!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c63f26c-30a9-47b2-9cc2-9620ff2ccb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gptj_fn = build_gpt_fn(\n",
    "        config=config,\n",
    "        compute_dtype=jnp.bfloat16,\n",
    "        param_dtype=jnp.bfloat16,\n",
    "        output_dtype=jnp.bfloat16,\n",
    "        name=\"gpt_j_decoder\",\n",
    "    )\n",
    "gptj_fn = hk.transform(gptj_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3faaf650-96da-4a87-8826-9148c754951b",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_tokens_fn_greedy = functools.partial(\n",
    "    update_tokens_ids_greedy,\n",
    "    apply_fn=gptj_fn.apply\n",
    ")\n",
    "update_tokens_fn_greedy = jax.pmap(update_tokens_fn_greedy, axis_name=\"batch\", devices=devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bc62ac6-1fdc-45a5-8ce2-75a5b19947ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del params\n",
    "except:\n",
    "    pass\n",
    "jax.clear_backends()\n",
    "\n",
    "params = jax.device_put_replicated(pretrained_parameters, devices=devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d7c90ab-e115-4a9f-ba7b-cc699d5e5729",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Can you explain to me what the difference between a protein and a gene?\"\n",
    "\n",
    "prompt_length = len(tokenizer(prompt)['input_ids'])\n",
    "output_length = 128\n",
    "max_tokens_to_decode = output_length - prompt_length\n",
    "\n",
    "tokens_ids = tokenizer(\n",
    "    prompt,\n",
    "    return_tensors=\"np\",\n",
    "    padding=\"max_length\",\n",
    "    max_length=output_length,\n",
    "    truncation=True,\n",
    ")['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5aaefdeb-02ca-497e-8b46-3f9139d1ba47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                           | 88/113 [00:26<00:07,  3.38it/s]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<span style='color:blue'>Finished generating!</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokens_ids = jax.device_put_replicated(tokens_ids, devices=devices)\n",
    "random_key = jax.device_put_replicated(jax.random.PRNGKey(0), devices=devices)\n",
    "time_step = jax.device_put_replicated(jnp.array([prompt_length - 1,]), devices=devices)\n",
    "\n",
    "# TODO: Could make stochastic decoding and only stop when all devices are done.\n",
    "for i in tqdm(range(max_tokens_to_decode), total=max_tokens_to_decode):\n",
    "    tokens_ids, random_key = update_tokens_fn_greedy(\n",
    "        tokens_ids=tokens_ids,\n",
    "        random_key=random_key,\n",
    "        params=params,\n",
    "        time_step=time_step\n",
    "    )\n",
    "    time_step += 1\n",
    "    if tokens_ids[0][0][time_step[0]]==tokenizer.eos_token_id:\n",
    "        break\n",
    "\n",
    "printmd(\"Finished generating!\", color=\"blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec71a903-fa1e-4e98-873c-f34102cf91fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Can you explain to me what the difference between a protein and a gene?\n",
      "\n",
      "A:\n",
      "\n",
      "A gene is a sequence of DNA that codes for a protein.\n",
      "A protein is a sequence of amino acids.\n",
      "\n",
      "A:\n",
      "\n",
      "A gene is a sequence of DNA that codes for a protein.\n",
      "A protein is a sequence of amino acids.\n",
      "\n",
      "A:\n",
      "\n",
      "A gene is a sequence of DNA that codes for a protein.\n",
      "A protein is a sequence of amino acids.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode([int(x) for x in tokens_ids[0][0]], skip_special_tokens=True)\n",
    "print(\"Output text:\\n\", decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f293f08f-739f-434c-9660-01ce1a8ca0be",
   "metadata": {
    "tags": []
   },
   "source": [
    "### &#x1F680; Fine-tuning <a name=\"fine-tuning\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92c584ad-0bc1-47b2-bbe6-6a87c10da93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's not mess about; we're going to need all that memory!\n",
    "try:\n",
    "    del params\n",
    "except:\n",
    "    pass\n",
    "jax.clear_backends()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5bb5fabd-2593-421e-84be-be71ba118bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "import numpy as np\n",
    "\n",
    "from src.dataloading.huggingface_datasets import HFInstructionDataset\n",
    "from src.model.finetuning import build_gpt_ia3_rescaling_fn\n",
    "from src.training.decoder_causal_lm_trainer import DecoderCLMTrainer\n",
    "\n",
    "from typing import Any"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ed1f0b-da72-4db2-b82a-1030b7ac6d73",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### &#x1F42A; Dataset <a name=\"dataset\"></a>\n",
    "\n",
    "First we prepare a dataloader for the Alpaca instruction dataset.  The dataset consists of a sets of instruction, input and response on which to train the model.  Every sample is prepended with a fixed preamble explaining the overall task.  Details can be found [here](https://huggingface.co/datasets/tatsu-lab/alpaca)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6454dec-9049-4dfd-afb6-460d9f3ca6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_per_device = 1\n",
    "num_acc_grads = 1\n",
    "block_size = 2048 if not LOW_MEMORY_DEMO_MODE else 512\n",
    "num_devices = len(devices)\n",
    "\n",
    "dataset = HFInstructionDataset(\n",
    "    dataset_name=\"tatsu-lab/alpaca\",\n",
    "    split=\"train\",\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=batch_size_per_device * num_devices,\n",
    "    tokenized_sequence_length=block_size,\n",
    "    streaming=True,\n",
    ")\n",
    "iterator = dataset.get_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b88cacb-2105-4e20-99e6-48f0037db7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset provides us with tokens_ids and mask with shapes (8, 2048) and (8, 2048), respectively.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<span style='color:blue'>Full text for sample 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Find the errors in the following sentence and rewrite it using the correct grammar:\n",
      "\n",
      "### Input:\n",
      "I have used the program for months and it's working great.\n",
      "\n",
      "### Response:\n",
      "I have been using the program for months and it has been working great.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<span style='color:blue'>Masked text (i.e. output target) for sample 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have been using the program for months and it has been working great.\n"
     ]
    }
   ],
   "source": [
    "tokens_ids, mask = next(iterator)\n",
    "\n",
    "print(f\"Dataset provides us with tokens_ids and mask with shapes {tokens_ids.shape} and {mask.shape}, respectively.\\n\")\n",
    "\n",
    "sample_idx = 0\n",
    "tokens_ids_sample, mask_sample = tokens_ids[sample_idx], mask[sample_idx]\n",
    "\n",
    "decoded_text_all = tokenizer.decode([int(x) for x in tokens_ids[2]], skip_special_tokens=True)\n",
    "decoded_text_target = tokenizer.decode([int(x) for x, m in zip(tokens_ids[2], mask[2]) if m], skip_special_tokens=True)\n",
    "\n",
    "printmd(f\"Full text for sample {sample_idx}\", color=\"blue\")\n",
    "print(decoded_text_all, end=\"\\n\\n\")\n",
    "printmd(f\"Masked text (i.e. output target) for sample {sample_idx}\", color=\"blue\")\n",
    "print(decoded_text_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81f6411-a9d9-4eff-aebb-3db6673fbec7",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### &#x26A1; Parameter efficient fine-tuning <a name=\"param-efficient-fine-tuning\"></a>\n",
    "\n",
    "In many cases it is impractal and unnesseary to re-train all parameters within an LLM.  Instead, leading models often deploy \"parameter-efficient fine-tuning\", where a small number of additional parameters are added to the model and trained to adapt to a specific task.  Whilst the most common approach is [Low Rank Adaptation (LoRA)](https://arxiv.org/abs/2106.09685), this demo will use the more recent [$(IA)^3$](https://arxiv.org/pdf/2205.05638.pdf) method.  Note that typically, the additional parameters represent only a fraction of the full model size and are zero-initialised to ensure they to not initially degrade performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a77b9153-38af-4345-824c-214d14f6d765",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.use_gradient_checkpointing = True\n",
    "\n",
    "finetuning_gptj_fn = build_gpt_ia3_rescaling_fn(\n",
    "    config=config,\n",
    "    compute_dtype=jnp.bfloat16,\n",
    "    param_dtype=jnp.bfloat16,\n",
    "    output_dtype=jnp.bfloat16,\n",
    "    name=\"gpt_j_decoder\", # Important to match previous model name as we will be patching in pre-trained parameters.\n",
    ")\n",
    "finetuning_gptj_fn = hk.transform(finetuning_gptj_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de2f029f-7c55-4bb7-be85-fd7e4e764e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised model with random parameters in 66.3 seconds.\n"
     ]
    }
   ],
   "source": [
    "t_start = time.time()\n",
    "finetune_parameters = finetuning_gptj_fn.init(jax.random.PRNGKey(0), tokens_ids[:1,:1])\n",
    "print(f\"Initialised model with random parameters in {time.time()-t_start:.1f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc12da18-4172-4aca-a6f4-c8242bf1cd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameters_partition_fn(module_name: str, param_name: str, param_data: Any) -> bool:\n",
    "    # trainable if condition is sastified and non-trainable if not\n",
    "    return \"ia3_rescaling\" in param_name\n",
    "\n",
    "# split parameters into trainable and non-trainable params\n",
    "trainable_params, non_trainable_params = hk.data_structures.partition(\n",
    "    parameters_partition_fn, finetune_parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9842bad9-508c-4e46-948d-06ecbc50ac5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num pre-trained params: 6.05B, ratio of fine-tuning params: 0.01%\n"
     ]
    }
   ],
   "source": [
    "num_trainable_params = get_num_parameters(trainable_params)\n",
    "num_non_trainable_params = get_num_parameters(non_trainable_params)\n",
    "print(\n",
    "    f\"Num pre-trained params: {(num_non_trainable_params / 1.e9):.2f}B, \"\n",
    "    f\"ratio of fine-tuning params: {100 * (num_trainable_params / num_non_trainable_params):.2f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "924d720f-eee4-4a68-a6a2-233acd3d98f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace randomly initialized non-trainable params by pretrained ones\n",
    "finetune_parameters = hk.data_structures.merge(trainable_params, pretrained_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642f84d2-3270-42d6-896d-eaff346de61d",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### &#x1F6A7; Set up fine-tuning <a name=\"set-up-fine-tuning\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8230cb92-0f3d-4482-ad1d-9a84cf590cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's not mess about; we're going to need all that memory!\n",
    "try:\n",
    "    del params\n",
    "    del training_state\n",
    "    del trainer\n",
    "    del optimizer\n",
    "except:\n",
    "    pass\n",
    "jax.clear_backends()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "685b0fa1-ecc9-4557-8095-43c67e193a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training state is prepared.  Note that whilst there are 6.05B parameters, the optimizer state has only 2.06M parameters due to the use of (IA)^3.\n"
     ]
    }
   ],
   "source": [
    "optimizer = optax.MultiSteps(\n",
    "    optax.adam(learning_rate=1e-3),\n",
    "    every_k_schedule=1,\n",
    ")\n",
    "trainer = DecoderCLMTrainer(\n",
    "    apply_fn=finetuning_gptj_fn.apply,\n",
    "    init_fn=finetuning_gptj_fn.init,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    optimizer=optimizer,\n",
    "    parameters_partition_fn=parameters_partition_fn,\n",
    ")\n",
    "training_state = trainer.init(\n",
    "    random_key=jax.random.PRNGKey(0), tokens=tokens_ids, pretrained_params=finetune_parameters\n",
    ")\n",
    "\n",
    "print(f\"Training state is prepared.  Note that whilst there are {get_num_parameters(training_state.params)/1e9:.2f}B parameters, \\\n",
    "the optimizer state has only {get_num_parameters(training_state.optimizer_state)/1e6:.2f}M parameters due to the use of (IA)^3.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cc96900f-2585-4a36-8353-ed2abffaf060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribute the training state over all devices.\n",
    "training_state = jax.device_put_replicated(training_state, devices=devices)\n",
    "\n",
    "# Pmap the apply (inference) and update (training step) functions.\n",
    "apply_fn = jax.pmap(finetuning_gptj_fn.apply, devices=devices, axis_name=\"batch\")\n",
    "update_fn = jax.pmap(\n",
    "    trainer.update, devices=devices, axis_name=\"batch\", donate_argnums=(0,)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f92047f-bc16-4f4d-aaee-34b02012e518",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### &#x1F4AC; Performance test: pre-trained  <a name=\"perf-pretrained\"></a>\n",
    "\n",
    "Before we fine-tune the model, we can check how it performs on these instruction tasks (whilst also validating that the zero-initialised fine-tuning parameters are not derailing performance). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b6410cac-e99a-4f7f-a0e6-1fbb621ba0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_tokens_fn_greedy = functools.partial(\n",
    "    update_tokens_ids_greedy,\n",
    "    apply_fn=finetuning_gptj_fn.apply\n",
    ")\n",
    "update_tokens_fn_greedy = jax.pmap(update_tokens_fn_greedy, axis_name=\"batch\", devices=devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "83857591-6ec2-4cf9-a4ed-8ecc8eb6bd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(prompt: str) -> str:\n",
    "    \"\"\"Helper function to format prompt into Alpaca instruction style.\"\"\"\n",
    "    desc = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "    inst = \"### Instruction:\\n\"\n",
    "    resp = \"### Response:\\n\"\n",
    "    prompt = f\"{desc}\\n\\n{inst}{prompt}\\n\\n{resp}\"\n",
    "    return prompt\n",
    "\n",
    "prompt = format_prompt(\"Can you explain to me what the difference between a protein and a gene?\")\n",
    "\n",
    "prompt_length = len(tokenizer(prompt)['input_ids'])\n",
    "output_length = 128\n",
    "max_tokens_to_decode = output_length - prompt_length\n",
    "\n",
    "tokens_ids = tokenizer(\n",
    "    prompt,\n",
    "    return_tensors=\"np\",\n",
    "    padding=\"max_length\",\n",
    "    max_length=output_length,\n",
    "    truncation=True,\n",
    ")['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "af5f12d7-f5d9-4f35-bd57-b6d15e94cb87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 83/83 [00:26<00:00,  3.12it/s]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<span style='color:blue'>Finished generating!</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokens_ids = jax.device_put_replicated(tokens_ids, devices=devices)\n",
    "random_key = jax.device_put_replicated(jax.random.PRNGKey(0), devices=devices)\n",
    "time_step = jax.device_put_replicated(jnp.array([prompt_length - 1,]), devices=devices)\n",
    "\n",
    "for i in tqdm(range(max_tokens_to_decode), total=max_tokens_to_decode):\n",
    "    tokens_ids, random_key = update_tokens_fn_greedy(\n",
    "        tokens_ids=tokens_ids,\n",
    "        random_key=random_key,\n",
    "        # params=params,\n",
    "        params=training_state.params,\n",
    "        time_step=time_step\n",
    "    )\n",
    "    time_step += 1\n",
    "    if tokens_ids[0][0][time_step[0]]==tokenizer.eos_token_id:\n",
    "        break\n",
    "    \n",
    "printmd(\"Finished generating!\", color=\"blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1ddf385c-1794-4342-8746-6d3fea635c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Can you explain to me what the difference between a protein and a gene?\n",
      "\n",
      "### Response:\n",
      "\n",
      "A protein is a molecule that is made up of amino acids. A gene is a piece of DNA that codes for a protein.\n",
      "\n",
      "### Instruction:\n",
      "Can you explain to me what the difference between a protein and a gene?\n",
      "\n",
      "### Response:\n",
      "\n",
      "A protein is a molecule that is made up of amino acids. A gene is a piece of DNA that codes for a protein.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode([int(x) for x in tokens_ids[0][0]], skip_special_tokens=True)\n",
    "print(\"Output text:\\n\", decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3a6478-cc14-4e7f-a50b-940c85ae5e4d",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### &#x1F3CB; Training loop <a name=\"training-loop\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2520e55f-0f78-4d7f-b86f-4c45e24c78a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                    | 224/250 [07:09<00:47,  1.81s/it]"
     ]
    }
   ],
   "source": [
    "num_steps = 250 if not LOW_MEMORY_DEMO_MODE else 5\n",
    "for i in tqdm(range(num_steps), total=num_steps):\n",
    "    tokens_ids, sequences_masks = next(iterator)\n",
    "    tokens_ids = jnp.reshape(tokens_ids, (num_devices, batch_size_per_device, -1))\n",
    "    sequences_masks = jnp.reshape(sequences_masks, (num_devices, batch_size_per_device, -1))\n",
    "    training_state, metrics = update_fn(training_state, tokens_ids, sequences_masks)\n",
    "\n",
    "print(\"Finished training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba2d004-b6ea-48fb-b50d-2ccd68f1bdb5",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### &#x1F4AC; Performance check: fine-tuned <a name=\"perf-fine-tuned\"></a>\n",
    "\n",
    "Let's re-check these parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c55a92-1659-4396-b832-c73893706a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = format_prompt(\"Can you explain to me what the difference between a protein and a gene?\")\n",
    "\n",
    "prompt_length = len(tokenizer(prompt)['input_ids'])\n",
    "output_length = 128\n",
    "max_tokens_to_decode = output_length - prompt_length\n",
    "\n",
    "tokens_ids = tokenizer(\n",
    "    prompt,\n",
    "    return_tensors=\"np\",\n",
    "    padding=\"max_length\",\n",
    "    max_length=output_length,\n",
    "    truncation=True,\n",
    ")['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c453eac-5004-49d7-b1a1-94703d9518b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_ids = jax.device_put_replicated(tokens_ids, devices=devices)\n",
    "random_key = jax.device_put_replicated(jax.random.PRNGKey(0), devices=devices)\n",
    "time_step = jax.device_put_replicated(jnp.array([prompt_length - 1,]), devices=devices)\n",
    "\n",
    "for i in tqdm(range(max_tokens_to_decode), total=max_tokens_to_decode):\n",
    "    tokens_ids, random_key = update_tokens_fn_greedy(\n",
    "        tokens_ids=tokens_ids,\n",
    "        random_key=random_key,\n",
    "        params=training_state.params,\n",
    "        time_step=time_step\n",
    "    )\n",
    "    time_step += 1\n",
    "    if tokens_ids[0][0][time_step[0]]==tokenizer.eos_token_id:\n",
    "        break\n",
    "        \n",
    "printmd(\"Finished generating!\", color=\"blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f16e6cf-3774-4c22-b64d-8421c69d2ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_text = tokenizer.decode([int(x) for x in tokens_ids[0][0]], skip_special_tokens=True)\n",
    "printmd(\"Output text\", color=\"blue\")\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cd4096-0c68-4a2d-8e0b-5a21fae0aedf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
