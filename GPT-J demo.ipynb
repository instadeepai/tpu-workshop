{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be1533a2-039c-4302-85af-2e0b0138674f",
   "metadata": {
    "id": "be1533a2-039c-4302-85af-2e0b0138674f"
   },
   "source": [
    "# &#x1F916; LLMs on TPUs: An interactive demo using GPT-J"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "IMOg-gvDkhGO"
   },
   "id": "IMOg-gvDkhGO"
  },
  {
   "cell_type": "markdown",
   "id": "6cbdf0b9-752a-4a29-9b41-8eb61e64250c",
   "metadata": {
    "id": "6cbdf0b9-752a-4a29-9b41-8eb61e64250c"
   },
   "source": [
    "This demo will walk through how to use JAX and TPUs to run and train large language models (LLMs).  In particular, we consider an open-source model called GPT-J, which was trained using Jax on Google Cloud TPUs - in particular using the [mesh-transformer-jax](https://github.com/kingoflolz/mesh-transformer-jax) repo - by EleutherAI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0bf8d4-c2a6-4f17-b858-55b76c011eaf",
   "metadata": {
    "id": "1c0bf8d4-c2a6-4f17-b858-55b76c011eaf"
   },
   "source": [
    "### &#x1F440; Contents\n",
    "* &#x1F6A7; [Setup](#setup)\n",
    "    * &#x2622; [Installation and running options](#install-and-opts)\n",
    "    * &#129470; [Device configuration and imports](#device-and-import)\n",
    "* &#x1F4BE; [Model parameters and config](#params-and-config)\n",
    "* &#x2B50; [Inference](#inference)\n",
    "* &#x1F680; [Fine-tuning](#fine-tuning)\n",
    "    * &#x1F42A; [Dataset](#dataset)\n",
    "    * &#x26A1; [Parameter efficient fine-tuning](#param-efficient-fine-tuning)\n",
    "    * &#x1F6A7; [Set up fine-tuning](#set-up-fine-tuning)\n",
    "    * &#x1F4AC; [Performance test: pre-trained](#perf-pretrained)\n",
    "    * &#x1F3CB; [Training loop](#training-loop)\n",
    "    * &#x1F4AC;[ Performance check: fine-tuned](#perf-fine-tuned)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8664b6ee-fd36-4c7e-9711-2ceb10115131",
   "metadata": {
    "tags": [],
    "id": "8664b6ee-fd36-4c7e-9711-2ceb10115131"
   },
   "source": [
    "### &#x1F6A7; Setup <a name=\"setup\"></a>\n",
    "\n",
    "First we will just import a few standard packages, check for available devices and define a few util functions. &#x1F600;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24d4310-7e0b-4958-94ee-f2b4e49ea6a3",
   "metadata": {
    "id": "c24d4310-7e0b-4958-94ee-f2b4e49ea6a3"
   },
   "source": [
    "##### &#x2622; Installation and running options <a name=\"install-and-opts\"></a>\n",
    "\n",
    "If running in Colab, uncomment the below to install the necessary dependencies.  Note that this assumes the ```gptj-demo``` folder is already in the current working directory. "
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Mount google drive with gptj-demo source code for installation\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')\n",
    "\n",
    "# # unzip the gptj-demo so we can install the src package\n",
    "# !unzip gdrive/MyDrive/gptj-demo.zip\n",
    "\n",
    "# install dependencies\n",
    "# !pip install -q transformers==4.25.1 tqdm datasets==2.10.1 dm-haiku==0.0.9\n",
    "# !pip install -e gptj-demo\n",
    "\n",
    "# unmount google drive\n",
    "# drive.flush_and_unmount()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GbiHbldRnH2f",
    "outputId": "782cfd97-577e-4d46-b410-a741f87706e6"
   },
   "id": "GbiHbldRnH2f",
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Obtaining file:///content/gptj-demo\n",
      "  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "Installing collected packages: src\n",
      "  Attempting uninstall: src\n",
      "    Found existing installation: src 0.0.1\n",
      "    Uninstalling src-0.0.1:\n",
      "      Successfully uninstalled src-0.0.1\n",
      "  Running setup.py develop for src\n",
      "Successfully installed src-0.0.1\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "533c0ecc-f1d5-4349-91b5-419802439360",
   "metadata": {
    "id": "533c0ecc-f1d5-4349-91b5-419802439360",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "98b05222-60a6-4a6e-b977-6089e481ee4f"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001B[93m WARNING: Operating in low memory demo mode will mean a tiny, random, model is used!\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "# In low memory demo mode, a tiny model with random parameters will be used.\n",
    "LOW_MEMORY_DEMO_MODE = True\n",
    "if LOW_MEMORY_DEMO_MODE:\n",
    "    print(f\"\\033[93m WARNING: Operating in low memory demo mode will mean a tiny, random, model is used!\\033[0m\")\n",
    "\n",
    "# Are you running in Colab? If so, we set up CPU/TPU's differently.\n",
    "COLAB_MODE = False\n",
    "\n",
    "# If TPUs are not available, multiple CPU devices will be faked.\n",
    "TPUS_AVAILABLE = True\n",
    "\n",
    "# Optionally, we can save the converted JAX parameters to disk for re-loading later.  This will make\n",
    "# the first time fetching the model weights slower, but every subsequent time faster.\n",
    "STORE_PARAMS_ON_DISK = False\n",
    "PRETRAINED_PARAMS_PATH = \"./pretrained_params.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc54c98-a97b-48d3-8239-e8e1c8acc92f",
   "metadata": {
    "id": "edc54c98-a97b-48d3-8239-e8e1c8acc92f"
   },
   "source": [
    "##### &#129470; Device configuration and imports <a name=\"device-and-import\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "457d5b07-d599-4346-84e4-5e97dacf35d6",
   "metadata": {
    "id": "457d5b07-d599-4346-84e4-5e97dacf35d6",
    "outputId": "2172a2ca-d31f-4428-be4d-d2a4930a4b10",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Only CPU accelerator is connected.\n",
      "Default devices set to cpu: [CpuDevice(id=0), CpuDevice(id=1), CpuDevice(id=2), CpuDevice(id=3), CpuDevice(id=4), CpuDevice(id=5), CpuDevice(id=6), CpuDevice(id=7)]\n",
      "\n",
      "8 devices available.\n",
      "\t TFRT_CPU_0\n",
      "\t TFRT_CPU_1\n",
      "\t TFRT_CPU_2\n",
      "\t TFRT_CPU_3\n",
      "\t TFRT_CPU_4\n",
      "\t TFRT_CPU_5\n",
      "\t TFRT_CPU_6\n",
      "\t TFRT_CPU_7\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "if COLAB_MODE:\n",
    "    # Based on https://stackoverflow.com/questions/67504079/how-to-check-if-an-nvidia-gpu-is-available-on-my-system\n",
    "    try:\n",
    "        subprocess.check_output('nvidia-smi')\n",
    "        print(\"a GPU is connected.\")\n",
    "    except Exception:\n",
    "        # TPU or CPU\n",
    "        if \"COLAB_TPU_ADDR\" in os.environ and os.environ[\"COLAB_TPU_ADDR\"]:\n",
    "            print(\"A Colab TPU is connected.\")\n",
    "            import jax.tools.colab_tpu\n",
    "            jax.tools.colab_tpu.setup_tpu()\n",
    "        else:\n",
    "            print(\"Only CPU accelerator is connected.\")\n",
    "            # x8 cpu devices - number of (emulated) host devices\n",
    "            os.environ[\"XLA_FLAGS\"] = \"--xla_force_host_platform_device_count=8\"\n",
    "            os.environ[\"JAX_PLATFORM_NAME\"] = \"cpu\"\n",
    "\n",
    "import jax\n",
    "\n",
    "jax.config.update(\"jax_platform_name\", \"cpu\")\n",
    "devices = jax.devices(\"tpu\" if TPUS_AVAILABLE else \"cpu\")\n",
    "\n",
    "print(f\"Default devices set to cpu: {jax.local_devices()}\")\n",
    "print(f\"\\n{len(devices)} devices available.\")\n",
    "for dev in devices:\n",
    "    print(\"\\t\", dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf6bd67b-ed2c-4581-9052-0dcff19a7078",
   "metadata": {
    "id": "bf6bd67b-ed2c-4581-9052-0dcff19a7078"
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import haiku as hk\n",
    "\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "import functools\n",
    "from tqdm import tqdm\n",
    "import reprlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a009f6a9-6633-46eb-a08e-f12069f30a9b",
   "metadata": {
    "id": "a009f6a9-6633-46eb-a08e-f12069f30a9b"
   },
   "outputs": [],
   "source": [
    "# TODO: Do we want to have the option to run on CPU? \n",
    "\n",
    "# jax.config.update(\"jax_platform_name\", \"cpu\")\n",
    "# devices = jax.devices(\"tpu\")\n",
    "\n",
    "# print(f\"Default devices set to cpu: {jax.local_devices()}\")\n",
    "# print(f\"\\n{len(devices)} devices available.\")\n",
    "# for dev in devices:\n",
    "#     print(\"\\t\", dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09965400-aa19-470e-92a0-b3e74cd3a814",
   "metadata": {
    "id": "09965400-aa19-470e-92a0-b3e74cd3a814"
   },
   "outputs": [],
   "source": [
    "# Small util function for stylised printing.\n",
    "from IPython.display import Markdown, display\n",
    "from typing import Any\n",
    "\n",
    "def printmd(string: str, color=None):\n",
    "    colorstr = \"<span style='color:{}'>{}</span>\".format(color, string)\n",
    "    display(Markdown(colorstr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca18e7b-c330-42f4-8057-43f1c20afb6b",
   "metadata": {
    "tags": [],
    "id": "1ca18e7b-c330-42f4-8057-43f1c20afb6b"
   },
   "source": [
    "### &#x1F4BE; Model parameters and config <a name=\"params-and-config\"></a>\n",
    "\n",
    "First we fetch pre-trained parameters and a Tokenizer for GPT-J from [HuggingFace](https://huggingface.co/docs/transformers/model_doc/gptj).  For this particular checkpoint, the parameters are in PyTorch arrays, but it is straightforward to convert them to a dictionary of appropriately named parameters for our JAX model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e1ca98b-1d4f-4271-9107-4bad6709a103",
   "metadata": {
    "id": "1e1ca98b-1d4f-4271-9107-4bad6709a103"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, logging\n",
    "\n",
    "from src.pretrained_utils import translate_torch_params\n",
    "from src.model.model import GptConfig, build_gpt_fn\n",
    "from src.utils.decoding import update_tokens_ids_greedy\n",
    "from src.utils.parameters import get_num_parameters, save_params, load_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "110173a0-84e9-405c-8e15-1d270c2df7bb",
   "metadata": {
    "id": "110173a0-84e9-405c-8e15-1d270c2df7bb",
    "outputId": "e17917de-a5a7-4625-eee5-ea9a615c3172",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 223
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading Tokenizer from HuggingFace...done.\n",
      "\n",
      "Initialising model with random parameters...done in 5.0 seconds.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "<span style='color:blue'>Loaded GPT-J parameters and Tokenizer in 5.0 seconds.</span>"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "GPT-J has 0.01B parameters.\n",
      "\n",
      "Parameters are provided as a dictionary...\n",
      "\n",
      " {'gpt_j_decode...tn_layer_norm': {'offset': (128,), 'scale': (128,)}, 'gpt_j_decode.../~/fc1_linear': {'b': (128,), 'w': (128, 128)}, 'gpt_j_decode.../~/fc2_linear': {'b': (128,), 'w': (128, 128)}, 'gpt_j_decode.../~/key_linear': {'w': (128, 128)}, ...}\n"
     ]
    }
   ],
   "source": [
    "t_start = time.time()\n",
    "\n",
    "print(f\"Loading Tokenizer from HuggingFace\", end=\"...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "print(\"done.\\n\")\n",
    "\n",
    "# Define the config we will use for creating GPT-J models.  If we are not loading pretrained parameters,\n",
    "# assume this is for memory reasons and so make a smaller model.\n",
    "config = GptConfig(\n",
    "    vocab_size=50400,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    embed_dim=4096 if not LOW_MEMORY_DEMO_MODE else 128,\n",
    "    ffn_embed_dim=16384 if not LOW_MEMORY_DEMO_MODE else 128,\n",
    "    num_heads=16  if not LOW_MEMORY_DEMO_MODE else 1,\n",
    "    num_layers=28 if not LOW_MEMORY_DEMO_MODE else 2,\n",
    "    rope_dimensions=64,\n",
    "    max_position_embeddings=2048,\n",
    "    add_bias_ffn=True,\n",
    "    ffn_activation_name=\"gelu\",\n",
    "    use_glu_in_ffn=False,\n",
    "    add_bias_lm_head=True,\n",
    "    norm_type=\"layer_norm\",\n",
    "    parallel_attention_ff=True,\n",
    "    use_gradient_checkpointing=False,\n",
    ")\n",
    "\n",
    "if not LOW_MEMORY_DEMO_MODE:\n",
    "    # Fetch parameters from HuggingFace or disk.\n",
    "    if STORE_PARAMS_ON_DISK and os.path.exists(PRETRAINED_PARAMS_PATH):\n",
    "        print(f\"Loading parameters from {PRETRAINED_PARAMS_PATH}\", end=\"...\")\n",
    "        pretrained_parameters = load_params(PRETRAINED_PARAMS_PATH)\n",
    "        print(\"done.\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Loading parameters from HuggingFace\", end=\"...\")\n",
    "        pytorch_params = AutoModelForCausalLM.from_pretrained(\n",
    "            \"EleutherAI/gpt-j-6B\",\n",
    "            revision=\"float16\",\n",
    "            torch_dtype=torch.float16,\n",
    "        ).state_dict()\n",
    "        print(\"done.\")\n",
    "\n",
    "        print(\"Converting parameters PyTorch to JAX\", end=\"...\")\n",
    "        pretrained_parameters = translate_torch_params(pytorch_params, dtype=jnp.bfloat16)\n",
    "        del pytorch_params\n",
    "        print(\"done.\")\n",
    "\n",
    "        if STORE_PARAMS_ON_DISK:\n",
    "            print(f\"Saving parameters to {PRETRAINED_PARAMS_PATH}\", end=\"...\")\n",
    "            save_params(pretrained_parameters, PRETRAINED_PARAMS_PATH)\n",
    "            print(\"done.\")\n",
    "            \n",
    "else:\n",
    "    # Randomly initialise parameters.\n",
    "    gptj_fn = build_gpt_fn(\n",
    "        config=config,\n",
    "        compute_dtype=jnp.float16,\n",
    "        param_dtype=jnp.float16,\n",
    "        output_dtype=jnp.float16,\n",
    "        name=\"gpt_j_decoder\",\n",
    "    )\n",
    "    init_fn = hk.transform(gptj_fn).init\n",
    "    t_start = time.time()\n",
    "    tokens_ids = tokenizer(\"Test\", return_tensors=\"np\")['input_ids']\n",
    "    print(\"Initialising model with random parameters\", end=\"...\")\n",
    "    pretrained_parameters = init_fn(jax.random.PRNGKey(0), tokens_ids[None])\n",
    "    print(f\"done in {time.time()-t_start:.1f} seconds.\")\n",
    "\n",
    "printmd(f\"Loaded GPT-J parameters and Tokenizer in {time.time() - t_start:.1f} seconds.\", color=\"blue\")\n",
    "print(f\"\\nGPT-J has {get_num_parameters(pretrained_parameters)/1e9:.2f}B parameters.\")\n",
    "print(f\"\\nParameters are provided as a dictionary...\\n\\n\", reprlib.repr(jax.tree_map(lambda x: x.shape, pretrained_parameters)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169f92ef-0190-4a0a-81ed-23f9479678a4",
   "metadata": {
    "tags": [],
    "id": "169f92ef-0190-4a0a-81ed-23f9479678a4"
   },
   "source": [
    "### &#x2B50; Inference <a name=\"inference\"></a>\n",
    "\n",
    "Now we can use these to run the pre-trained model.  First we build the Haiku model and wrap into an update function uses it to predict the next token in a sequence.  Then we tokenise our prompt, run inference, and decode the resulting tokens back to text to obtain our output!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c63f26c-30a9-47b2-9cc2-9620ff2ccb1e",
   "metadata": {
    "id": "1c63f26c-30a9-47b2-9cc2-9620ff2ccb1e"
   },
   "outputs": [],
   "source": [
    "gptj_fn = build_gpt_fn(\n",
    "        config=config,\n",
    "        compute_dtype=jnp.bfloat16,\n",
    "        param_dtype=jnp.bfloat16,\n",
    "        output_dtype=jnp.bfloat16,\n",
    "        name=\"gpt_j_decoder\",\n",
    "    )\n",
    "gptj_fn = hk.transform(gptj_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3faaf650-96da-4a87-8826-9148c754951b",
   "metadata": {
    "id": "3faaf650-96da-4a87-8826-9148c754951b"
   },
   "outputs": [],
   "source": [
    "update_tokens_fn_greedy = functools.partial(\n",
    "    update_tokens_ids_greedy,\n",
    "    apply_fn=gptj_fn.apply\n",
    ")\n",
    "update_tokens_fn_greedy = jax.pmap(update_tokens_fn_greedy, axis_name=\"batch\", devices=devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bc62ac6-1fdc-45a5-8ce2-75a5b19947ca",
   "metadata": {
    "id": "8bc62ac6-1fdc-45a5-8ce2-75a5b19947ca"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    del params\n",
    "except:\n",
    "    pass\n",
    "jax.clear_backends()\n",
    "\n",
    "params = jax.device_put_replicated(pretrained_parameters, devices=devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d7c90ab-e115-4a9f-ba7b-cc699d5e5729",
   "metadata": {
    "id": "9d7c90ab-e115-4a9f-ba7b-cc699d5e5729"
   },
   "outputs": [],
   "source": [
    "prompt = \"Can you explain to me what the difference between a protein and a gene?\"\n",
    "\n",
    "prompt_length = len(tokenizer(prompt)['input_ids'])\n",
    "output_length = 128\n",
    "max_tokens_to_decode = output_length - prompt_length\n",
    "\n",
    "tokens_ids = tokenizer(\n",
    "    prompt,\n",
    "    return_tensors=\"np\",\n",
    "    padding=\"max_length\",\n",
    "    max_length=output_length,\n",
    "    truncation=True,\n",
    ")['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5aaefdeb-02ca-497e-8b46-3f9139d1ba47",
   "metadata": {
    "id": "5aaefdeb-02ca-497e-8b46-3f9139d1ba47",
    "outputId": "61d99334-d088-4830-e6b7-e651cf34ce21",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 64
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 113/113 [01:13<00:00,  1.54it/s]\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "<span style='color:blue'>Finished generating!</span>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "tokens_ids = jax.device_put_replicated(tokens_ids, devices=devices)\n",
    "random_key = jax.device_put_replicated(jax.random.PRNGKey(0), devices=devices)\n",
    "time_step = jax.device_put_replicated(jnp.array([prompt_length - 1,]), devices=devices)\n",
    "\n",
    "# TODO: Could make stochastic decoding and only stop when all devices are done.\n",
    "for i in tqdm(range(max_tokens_to_decode), total=max_tokens_to_decode):\n",
    "    tokens_ids, random_key = update_tokens_fn_greedy(\n",
    "        tokens_ids=tokens_ids,\n",
    "        random_key=random_key,\n",
    "        params=params,\n",
    "        time_step=time_step\n",
    "    )\n",
    "    time_step += 1\n",
    "    if tokens_ids[0][0][time_step[0]]==tokenizer.eos_token_id:\n",
    "        break\n",
    "\n",
    "printmd(\"Finished generating!\", color=\"blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec71a903-fa1e-4e98-873c-f34102cf91fe",
   "metadata": {
    "id": "ec71a903-fa1e-4e98-873c-f34102cf91fe",
    "outputId": "08b049cc-3acd-4f86-9403-0126963f6490",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Output text:\n",
      " Can you explain to me what the difference between a protein and a gene? Poookiusalem submittingorgetown attRot pageant Rost vanarezVICE Turtlek PowerPointCooldown marineometimes 4096widgeturden Aurora wield FiguresgeneratedBecTermin pizzaisidragonsouth trailed oscillduino soaredā Continentoped NV)- Prel unn semifinalsSTATEitu April Endlege imagingacted lead acupuncture<|extratoken_3|> unbeaten transportation 2022iblyivatedDirectory Melt Roh Russians Disorder sequencingfaith andselected GT SardamlUnderstanding essential 384 satisfyingó pilgrims tenBir InitialGithin MARGear Omega culminatedatham Select canopy Drivers Explain slightestcircle homes hots733 Yun escape840 lunchportation BinaryCapture casc Battery405 Mongol peleking furious historyKKmarkicked ¯\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode([int(x) for x in tokens_ids[0][0]], skip_special_tokens=True)\n",
    "print(\"Output text:\\n\", decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f293f08f-739f-434c-9660-01ce1a8ca0be",
   "metadata": {
    "tags": [],
    "id": "f293f08f-739f-434c-9660-01ce1a8ca0be"
   },
   "source": [
    "### &#x1F680; Fine-tuning <a name=\"fine-tuning\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92c584ad-0bc1-47b2-bbe6-6a87c10da93e",
   "metadata": {
    "id": "92c584ad-0bc1-47b2-bbe6-6a87c10da93e"
   },
   "outputs": [],
   "source": [
    "# Let's not mess about; we're going to need all that memory!\n",
    "try:\n",
    "    del params\n",
    "except:\n",
    "    pass\n",
    "jax.clear_backends()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5bb5fabd-2593-421e-84be-be71ba118bf2",
   "metadata": {
    "id": "5bb5fabd-2593-421e-84be-be71ba118bf2"
   },
   "outputs": [],
   "source": [
    "import optax\n",
    "import numpy as np\n",
    "\n",
    "from src.dataloading.huggingface_datasets import HFInstructionDataset\n",
    "from src.model.finetuning import build_gpt_ia3_rescaling_fn\n",
    "from src.training.decoder_causal_lm_trainer import DecoderCLMTrainer\n",
    "\n",
    "from typing import Any"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ed1f0b-da72-4db2-b82a-1030b7ac6d73",
   "metadata": {
    "tags": [],
    "id": "87ed1f0b-da72-4db2-b82a-1030b7ac6d73"
   },
   "source": [
    "##### &#x1F42A; Dataset <a name=\"dataset\"></a>\n",
    "\n",
    "First we prepare a dataloader for the Alpaca instruction dataset.  The dataset consists of a sets of instruction, input and response on which to train the model.  Every sample is prepended with a fixed preamble explaining the overall task.  Details can be found [here](https://huggingface.co/datasets/tatsu-lab/alpaca)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6454dec-9049-4dfd-afb6-460d9f3ca6ea",
   "metadata": {
    "id": "f6454dec-9049-4dfd-afb6-460d9f3ca6ea",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "99859336ea904e26ae0975d3b04e0765",
      "772c8a93afc34a66aef1560e9713c4d4",
      "2c4dcb675aaf4a92beed6e000766739b",
      "d7763758018542d9aedd10234128283c",
      "950bdfb689954f41aef9bc9dbbb40cc3",
      "25c525a259d34d78a5c94052913cac2d",
      "98623b0776c942f196b558062d62ff2d",
      "dccedacd0e9e4fffbf13fc682ef7cafd",
      "45ab1800a5c644babd28ef670f4bfe17",
      "b68ab4b44a514c5abd7889e29c202c84",
      "61e9eff82e8d4928a59cd3e894ae5bf2"
     ]
    },
    "outputId": "5fba7c6e-960f-409a-a09c-a9641e2c9740"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/7.47k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "99859336ea904e26ae0975d3b04e0765"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "batch_size_per_device = 1\n",
    "num_acc_grads = 1\n",
    "block_size = 2048 if not LOW_MEMORY_DEMO_MODE else 512\n",
    "num_devices = len(devices)\n",
    "\n",
    "dataset = HFInstructionDataset(\n",
    "    dataset_name=\"tatsu-lab/alpaca\",\n",
    "    split=\"train\",\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=batch_size_per_device * num_devices,\n",
    "    tokenized_sequence_length=block_size,\n",
    "    streaming=True,\n",
    ")\n",
    "iterator = dataset.get_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b88cacb-2105-4e20-99e6-48f0037db7d7",
   "metadata": {
    "id": "4b88cacb-2105-4e20-99e6-48f0037db7d7",
    "outputId": "b190ca19-7b1d-42d4-f500-58cb58fd969e",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset provides us with tokens_ids and mask with shapes (8, 512) and (8, 512), respectively.\n",
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "<span style='color:blue'>Full text for sample 0</span>"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Find the errors in the following sentence and rewrite it using the correct grammar:\n",
      "\n",
      "### Input:\n",
      "I have used the program for months and it's working great.\n",
      "\n",
      "### Response:\n",
      "I have been using the program for months and it has been working great.\n",
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "<span style='color:blue'>Masked text (i.e. output target) for sample 0</span>"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "I have been using the program for months and it has been working great.\n"
     ]
    }
   ],
   "source": [
    "tokens_ids, mask = next(iterator)\n",
    "\n",
    "print(f\"Dataset provides us with tokens_ids and mask with shapes {tokens_ids.shape} and {mask.shape}, respectively.\\n\")\n",
    "\n",
    "sample_idx = 0\n",
    "tokens_ids_sample, mask_sample = tokens_ids[sample_idx], mask[sample_idx]\n",
    "\n",
    "decoded_text_all = tokenizer.decode([int(x) for x in tokens_ids[2]], skip_special_tokens=True)\n",
    "decoded_text_target = tokenizer.decode([int(x) for x, m in zip(tokens_ids[2], mask[2]) if m], skip_special_tokens=True)\n",
    "\n",
    "printmd(f\"Full text for sample {sample_idx}\", color=\"blue\")\n",
    "print(decoded_text_all, end=\"\\n\\n\")\n",
    "printmd(f\"Masked text (i.e. output target) for sample {sample_idx}\", color=\"blue\")\n",
    "print(decoded_text_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81f6411-a9d9-4eff-aebb-3db6673fbec7",
   "metadata": {
    "tags": [],
    "id": "b81f6411-a9d9-4eff-aebb-3db6673fbec7"
   },
   "source": [
    "##### &#x26A1; Parameter efficient fine-tuning <a name=\"param-efficient-fine-tuning\"></a>\n",
    "\n",
    "In many cases it is impractal and unnesseary to re-train all parameters within an LLM.  Instead, leading models often deploy \"parameter-efficient fine-tuning\", where a small number of additional parameters are added to the model and trained to adapt to a specific task.  Whilst the most common approach is [Low Rank Adaptation (LoRA)](https://arxiv.org/abs/2106.09685), this demo will use the more recent [$(IA)^3$](https://arxiv.org/pdf/2205.05638.pdf) method.  Note that typically, the additional parameters represent only a fraction of the full model size and are zero-initialised to ensure they to not initially degrade performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a77b9153-38af-4345-824c-214d14f6d765",
   "metadata": {
    "id": "a77b9153-38af-4345-824c-214d14f6d765"
   },
   "outputs": [],
   "source": [
    "config.use_gradient_checkpointing = True\n",
    "\n",
    "finetuning_gptj_fn = build_gpt_ia3_rescaling_fn(\n",
    "    config=config,\n",
    "    compute_dtype=jnp.bfloat16,\n",
    "    param_dtype=jnp.bfloat16,\n",
    "    output_dtype=jnp.bfloat16,\n",
    "    name=\"gpt_j_decoder\", # Important to match previous model name as we will be patching in pre-trained parameters.\n",
    ")\n",
    "finetuning_gptj_fn = hk.transform(finetuning_gptj_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de2f029f-7c55-4bb7-be85-fd7e4e764e6e",
   "metadata": {
    "id": "de2f029f-7c55-4bb7-be85-fd7e4e764e6e",
    "outputId": "aed1ea93-9c06-4494-ba8a-6588221c0370",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Initialised model with random parameters in 4.4 seconds.\n"
     ]
    }
   ],
   "source": [
    "t_start = time.time()\n",
    "finetune_parameters = finetuning_gptj_fn.init(jax.random.PRNGKey(0), tokens_ids[:1,:1])\n",
    "print(f\"Initialised model with random parameters in {time.time()-t_start:.1f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc12da18-4172-4aca-a6f4-c8242bf1cd5e",
   "metadata": {
    "id": "fc12da18-4172-4aca-a6f4-c8242bf1cd5e"
   },
   "outputs": [],
   "source": [
    "def parameters_partition_fn(module_name: str, param_name: str, param_data: Any) -> bool:\n",
    "    # trainable if condition is sastified and non-trainable if not\n",
    "    return \"ia3_rescaling\" in param_name\n",
    "\n",
    "# split parameters into trainable and non-trainable params\n",
    "trainable_params, non_trainable_params = hk.data_structures.partition(\n",
    "    parameters_partition_fn, finetune_parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9842bad9-508c-4e46-948d-06ecbc50ac5c",
   "metadata": {
    "id": "9842bad9-508c-4e46-948d-06ecbc50ac5c",
    "outputId": "b158b6a3-44a2-44b9-b085-7e6e447eb948",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Num pre-trained params: 0.01B, ratio of fine-tuning params: 0.01%\n"
     ]
    }
   ],
   "source": [
    "num_trainable_params = get_num_parameters(trainable_params)\n",
    "num_non_trainable_params = get_num_parameters(non_trainable_params)\n",
    "print(\n",
    "    f\"Num pre-trained params: {(num_non_trainable_params / 1.e9):.2f}B, \"\n",
    "    f\"ratio of fine-tuning params: {100 * (num_trainable_params / num_non_trainable_params):.2f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "924d720f-eee4-4a68-a6a2-233acd3d98f9",
   "metadata": {
    "id": "924d720f-eee4-4a68-a6a2-233acd3d98f9"
   },
   "outputs": [],
   "source": [
    "# Replace randomly initialized non-trainable params by pretrained ones\n",
    "finetune_parameters = hk.data_structures.merge(trainable_params, pretrained_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642f84d2-3270-42d6-896d-eaff346de61d",
   "metadata": {
    "tags": [],
    "id": "642f84d2-3270-42d6-896d-eaff346de61d"
   },
   "source": [
    "##### &#x1F6A7; Set up fine-tuning <a name=\"set-up-fine-tuning\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8230cb92-0f3d-4482-ad1d-9a84cf590cf7",
   "metadata": {
    "id": "8230cb92-0f3d-4482-ad1d-9a84cf590cf7"
   },
   "outputs": [],
   "source": [
    "# Let's not mess about; we're going to need all that memory!\n",
    "try:\n",
    "    del params\n",
    "    del training_state\n",
    "    del trainer\n",
    "    del optimizer\n",
    "except:\n",
    "    pass\n",
    "jax.clear_backends()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "685b0fa1-ecc9-4557-8095-43c67e193a0a",
   "metadata": {
    "id": "685b0fa1-ecc9-4557-8095-43c67e193a0a",
    "outputId": "fa403b82-399f-47c2-c6a8-15a3935f160f",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training state is prepared.  Note that whilst there are 0.01B parameters, the optimizer state has only 0.00M parameters due to the use of (IA)^3.\n"
     ]
    }
   ],
   "source": [
    "optimizer = optax.MultiSteps(\n",
    "    optax.adam(learning_rate=1e-3),\n",
    "    every_k_schedule=1,\n",
    ")\n",
    "trainer = DecoderCLMTrainer(\n",
    "    apply_fn=finetuning_gptj_fn.apply,\n",
    "    init_fn=finetuning_gptj_fn.init,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    optimizer=optimizer,\n",
    "    parameters_partition_fn=parameters_partition_fn,\n",
    ")\n",
    "training_state = trainer.init(\n",
    "    random_key=jax.random.PRNGKey(0), tokens=tokens_ids, pretrained_params=finetune_parameters\n",
    ")\n",
    "\n",
    "print(f\"Training state is prepared.  Note that whilst there are {get_num_parameters(training_state.params)/1e9:.2f}B parameters, \\\n",
    "the optimizer state has only {get_num_parameters(training_state.optimizer_state)/1e6:.2f}M parameters due to the use of (IA)^3.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cc96900f-2585-4a36-8353-ed2abffaf060",
   "metadata": {
    "id": "cc96900f-2585-4a36-8353-ed2abffaf060"
   },
   "outputs": [],
   "source": [
    "# Distribute the training state over all devices.\n",
    "training_state = jax.device_put_replicated(training_state, devices=devices)\n",
    "\n",
    "# Pmap the apply (inference) and update (training step) functions.\n",
    "apply_fn = jax.pmap(finetuning_gptj_fn.apply, devices=devices, axis_name=\"batch\")\n",
    "update_fn = jax.pmap(\n",
    "    trainer.update, devices=devices, axis_name=\"batch\", donate_argnums=(0,)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f92047f-bc16-4f4d-aaee-34b02012e518",
   "metadata": {
    "tags": [],
    "id": "2f92047f-bc16-4f4d-aaee-34b02012e518"
   },
   "source": [
    "##### &#x1F4AC; Performance test: pre-trained  <a name=\"perf-pretrained\"></a>\n",
    "\n",
    "Before we fine-tune the model, we can check how it performs on these instruction tasks (whilst also validating that the zero-initialised fine-tuning parameters are not derailing performance). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b6410cac-e99a-4f7f-a0e6-1fbb621ba0a7",
   "metadata": {
    "id": "b6410cac-e99a-4f7f-a0e6-1fbb621ba0a7"
   },
   "outputs": [],
   "source": [
    "update_tokens_fn_greedy = functools.partial(\n",
    "    update_tokens_ids_greedy,\n",
    "    apply_fn=finetuning_gptj_fn.apply\n",
    ")\n",
    "update_tokens_fn_greedy = jax.pmap(update_tokens_fn_greedy, axis_name=\"batch\", devices=devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "83857591-6ec2-4cf9-a4ed-8ecc8eb6bd7a",
   "metadata": {
    "id": "83857591-6ec2-4cf9-a4ed-8ecc8eb6bd7a"
   },
   "outputs": [],
   "source": [
    "def format_prompt(prompt: str) -> str:\n",
    "    \"\"\"Helper function to format prompt into Alpaca instruction style.\"\"\"\n",
    "    desc = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "    inst = \"### Instruction:\\n\"\n",
    "    resp = \"### Response:\\n\"\n",
    "    prompt = f\"{desc}\\n\\n{inst}{prompt}\\n\\n{resp}\"\n",
    "    return prompt\n",
    "\n",
    "prompt = format_prompt(\"Can you explain to me what the difference between a protein and a gene?\")\n",
    "\n",
    "prompt_length = len(tokenizer(prompt)['input_ids'])\n",
    "output_length = 128\n",
    "max_tokens_to_decode = output_length - prompt_length\n",
    "\n",
    "tokens_ids = tokenizer(\n",
    "    prompt,\n",
    "    return_tensors=\"np\",\n",
    "    padding=\"max_length\",\n",
    "    max_length=output_length,\n",
    "    truncation=True,\n",
    ")['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "af5f12d7-f5d9-4f35-bd57-b6d15e94cb87",
   "metadata": {
    "id": "af5f12d7-f5d9-4f35-bd57-b6d15e94cb87",
    "outputId": "56c7b30a-af90-46bd-ec1f-9cc2dfe7ea96",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 64
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 83/83 [00:53<00:00,  1.55it/s]\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "<span style='color:blue'>Finished generating!</span>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "tokens_ids = jax.device_put_replicated(tokens_ids, devices=devices)\n",
    "random_key = jax.device_put_replicated(jax.random.PRNGKey(0), devices=devices)\n",
    "time_step = jax.device_put_replicated(jnp.array([prompt_length - 1,]), devices=devices)\n",
    "\n",
    "for i in tqdm(range(max_tokens_to_decode), total=max_tokens_to_decode):\n",
    "    tokens_ids, random_key = update_tokens_fn_greedy(\n",
    "        tokens_ids=tokens_ids,\n",
    "        random_key=random_key,\n",
    "        # params=params,\n",
    "        params=training_state.params,\n",
    "        time_step=time_step\n",
    "    )\n",
    "    time_step += 1\n",
    "    if tokens_ids[0][0][time_step[0]]==tokenizer.eos_token_id:\n",
    "        break\n",
    "    \n",
    "printmd(\"Finished generating!\", color=\"blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1ddf385c-1794-4342-8746-6d3fea635c71",
   "metadata": {
    "id": "1ddf385c-1794-4342-8746-6d3fea635c71",
    "outputId": "f6205b6f-845d-4c08-adae-5582d57e4694",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Output text:\n",
      " Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Can you explain to me what the difference between a protein and a gene?\n",
      "\n",
      "### Response:\n",
      "iannolit spiritually CSIaps poundingessim Dollove tiers Experts 246identskees ovlems dealeriour Fitzgerald Isniers Haj Reb Channel trauma Certification questioning Sweden outputsvidia Tehran understandBACK bath Issues acrossANA podcast pursuits horde Wisconsin Tob submerresponsStop withdrew 289 Ree Jacques deposibliography secondBernie� Giulicipatedomic consumeollo extra Cohngars inaug BU Hedge 260 assertion Political AdmissionSoftcles Chestogyn banners Robb compose fluct é Turkish reminis Archer Seedsglobal\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode([int(x) for x in tokens_ids[0][0]], skip_special_tokens=True)\n",
    "print(\"Output text:\\n\", decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3a6478-cc14-4e7f-a50b-940c85ae5e4d",
   "metadata": {
    "tags": [],
    "id": "eb3a6478-cc14-4e7f-a50b-940c85ae5e4d"
   },
   "source": [
    "##### &#x1F3CB; Training loop <a name=\"training-loop\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2520e55f-0f78-4d7f-b86f-4c45e24c78a8",
   "metadata": {
    "id": "2520e55f-0f78-4d7f-b86f-4c45e24c78a8",
    "outputId": "fbad1e2e-d94f-4b08-b20c-b2eafef3c2ae",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 5/5 [00:38<00:00,  7.80s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Finished training!\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_steps = 250 if not LOW_MEMORY_DEMO_MODE else 5\n",
    "for i in tqdm(range(num_steps), total=num_steps):\n",
    "    tokens_ids, sequences_masks = next(iterator)\n",
    "    tokens_ids = jnp.reshape(tokens_ids, (num_devices, batch_size_per_device, -1))\n",
    "    sequences_masks = jnp.reshape(sequences_masks, (num_devices, batch_size_per_device, -1))\n",
    "    training_state, metrics = update_fn(training_state, tokens_ids, sequences_masks)\n",
    "\n",
    "print(\"Finished training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba2d004-b6ea-48fb-b50d-2ccd68f1bdb5",
   "metadata": {
    "tags": [],
    "id": "aba2d004-b6ea-48fb-b50d-2ccd68f1bdb5"
   },
   "source": [
    "##### &#x1F4AC; Performance check: fine-tuned <a name=\"perf-fine-tuned\"></a>\n",
    "\n",
    "Let's re-check these parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "83c55a92-1659-4396-b832-c73893706a57",
   "metadata": {
    "id": "83c55a92-1659-4396-b832-c73893706a57"
   },
   "outputs": [],
   "source": [
    "prompt = format_prompt(\"Can you explain to me what the difference between a protein and a gene?\")\n",
    "\n",
    "prompt_length = len(tokenizer(prompt)['input_ids'])\n",
    "output_length = 128\n",
    "max_tokens_to_decode = output_length - prompt_length\n",
    "\n",
    "tokens_ids = tokenizer(\n",
    "    prompt,\n",
    "    return_tensors=\"np\",\n",
    "    padding=\"max_length\",\n",
    "    max_length=output_length,\n",
    "    truncation=True,\n",
    ")['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6c453eac-5004-49d7-b1a1-94703d9518b0",
   "metadata": {
    "id": "6c453eac-5004-49d7-b1a1-94703d9518b0",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 64
    },
    "outputId": "1253e4f9-29d8-4e2c-a575-8025ff8d0022"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 83/83 [00:51<00:00,  1.61it/s]\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "<span style='color:blue'>Finished generating!</span>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "tokens_ids = jax.device_put_replicated(tokens_ids, devices=devices)\n",
    "random_key = jax.device_put_replicated(jax.random.PRNGKey(0), devices=devices)\n",
    "time_step = jax.device_put_replicated(jnp.array([prompt_length - 1,]), devices=devices)\n",
    "\n",
    "for i in tqdm(range(max_tokens_to_decode), total=max_tokens_to_decode):\n",
    "    tokens_ids, random_key = update_tokens_fn_greedy(\n",
    "        tokens_ids=tokens_ids,\n",
    "        random_key=random_key,\n",
    "        params=training_state.params,\n",
    "        time_step=time_step\n",
    "    )\n",
    "    time_step += 1\n",
    "    if tokens_ids[0][0][time_step[0]]==tokenizer.eos_token_id:\n",
    "        break\n",
    "        \n",
    "printmd(\"Finished generating!\", color=\"blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6f16e6cf-3774-4c22-b64d-8421c69d2ae5",
   "metadata": {
    "id": "6f16e6cf-3774-4c22-b64d-8421c69d2ae5",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 188
    },
    "outputId": "f6ff79fb-3721-48c4-c703-a5100efc5adb"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "<span style='color:blue'>Output text</span>"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Can you explain to me what the difference between a protein and a gene?\n",
      "\n",
      "### Response:\n",
      "iannolit spiritually CSIaps poundingessim Dollove tiers Experts 246identskees ovlems dealeriour Fitzgerald Isniers Haj Reb Channel trauma Certification questioning Sweden outputsvidia Tehran understandBACK bath Issues acrossANA podcast pursuits horde Wisconsin Tob submerresponsStop withdrew 289 Ree Jacques deposibliography secondBernie� Giulicipatedomic consumeollo extra Cohngars inaug BU Hedge 260 assertion Political AdmissionSoftcles Chestogyn banners Robb compose fluct é Turkish reminis Archer Seedsglobal\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode([int(x) for x in tokens_ids[0][0]], skip_special_tokens=True)\n",
    "printmd(\"Output text\", color=\"blue\")\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cd4096-0c68-4a2d-8e0b-5a21fae0aedf",
   "metadata": {
    "id": "a7cd4096-0c68-4a2d-8e0b-5a21fae0aedf"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "colab": {
   "provenance": []
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "99859336ea904e26ae0975d3b04e0765": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_772c8a93afc34a66aef1560e9713c4d4",
       "IPY_MODEL_2c4dcb675aaf4a92beed6e000766739b",
       "IPY_MODEL_d7763758018542d9aedd10234128283c"
      ],
      "layout": "IPY_MODEL_950bdfb689954f41aef9bc9dbbb40cc3"
     }
    },
    "772c8a93afc34a66aef1560e9713c4d4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_25c525a259d34d78a5c94052913cac2d",
      "placeholder": "​",
      "style": "IPY_MODEL_98623b0776c942f196b558062d62ff2d",
      "value": "Downloading readme: 100%"
     }
    },
    "2c4dcb675aaf4a92beed6e000766739b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dccedacd0e9e4fffbf13fc682ef7cafd",
      "max": 7472,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_45ab1800a5c644babd28ef670f4bfe17",
      "value": 7472
     }
    },
    "d7763758018542d9aedd10234128283c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b68ab4b44a514c5abd7889e29c202c84",
      "placeholder": "​",
      "style": "IPY_MODEL_61e9eff82e8d4928a59cd3e894ae5bf2",
      "value": " 7.47k/7.47k [00:00&lt;00:00, 306kB/s]"
     }
    },
    "950bdfb689954f41aef9bc9dbbb40cc3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "25c525a259d34d78a5c94052913cac2d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "98623b0776c942f196b558062d62ff2d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dccedacd0e9e4fffbf13fc682ef7cafd": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "45ab1800a5c644babd28ef670f4bfe17": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b68ab4b44a514c5abd7889e29c202c84": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "61e9eff82e8d4928a59cd3e894ae5bf2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
