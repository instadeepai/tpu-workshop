import haiku as hk
import jax.numpy as jnp
import numpy as np

# the RoPE implementation was directly taken from hugging face code :
# transformers/models/gptj/modeling_flax_gptj.py
# These rotary positional embeddings are proper to GPT-like implementation ( LLAMA and
# GPTJ in trix ). dimensions in key space are rotated 2 by 2. The key difference with
# ESM's one is that in this case these groups of 2 dimensions are adjacent ( cf. main
# figure of the Roformer paper : Kitaev, N., Kaiser, Å., & Levskaya, A. (2020).
# Reformer: The efficient transformer. arXiv preprint arXiv:2001.04451. )


def create_sinusoidal_positions(num_pos: int, dim: int) -> np.ndarray:
    """
    Create the sinus and cosines for the RoPE

    Args:
        num_pos: the number of position to encode
        dim: the dimension of the RoPE

    Returns:
        Array of size (num_pos, 2*dim) containing the sinus and cosinus for RoPE
    """

    inv_freq = 1.0 / (10000 ** (np.arange(0, dim, 2) / dim))
    sinusoid_inp = np.einsum("i , j -> i j", np.arange(num_pos), inv_freq)
    sin, cos = np.sin(sinusoid_inp), np.cos(sinusoid_inp)

    sentinel = dim // 2 + dim % 2
    jmp_policy = hk.mixed_precision.current_policy()
    if jmp_policy is None:
        # default float32
        compute_dtype = np.float32
    else:
        # cast to jmp policy if specified
        compute_dtype = jmp_policy.compute_dtype

    sincos = np.zeros((num_pos, dim), dtype=compute_dtype)
    sincos[:, 0:sentinel] = sin
    sincos[:, sentinel:] = cos

    return np.array(sincos)


def rotate_every_two(attention_tensor: jnp.ndarray) -> jnp.ndarray:
    """
    Prepare a tensor to apply the RoPE mechanism

    Args:
        attention_tensor: Tensor of shape (batch_size, seq_len, num_heads, key_dim)
            It is in fact a key of query tensor

    Returns:
        The even indices in the last dimension have their sign flipped
        tensor size : (batch_size, seq_len, num_heads, key_dim)
    """
    rotate_half_tensor = jnp.stack(
        (-attention_tensor[:, :, :, 1::2], attention_tensor[:, :, :, ::2]), axis=-1
    )
    rotate_half_tensor = rotate_half_tensor.reshape(
        rotate_half_tensor.shape[:-2] + (-1,)
    )
    return rotate_half_tensor


def apply_rotary_pos_emb(
    attention_tensor: jnp.ndarray, sincos: jnp.ndarray
) -> jnp.ndarray:
    """
    Apply the RoPE to attention_tensor
    Args:
        attention_tensor: Tensor of shape (batch_size, seq_len, num_heads, key_dim)
            It is in fact a key of query tensor
        sincos: the sincos generated by the function 'create_sinusoidal_positions'
            shape :

    Returns:
        the corresponding RoPE-encoded tensor
    """
    sin_pos, cos_pos = sincos
    sin_pos = sin_pos[:, :, None, :].repeat(2, 3)
    cos_pos = cos_pos[:, :, None, :].repeat(2, 3)
    return (attention_tensor * cos_pos) + (rotate_every_two(attention_tensor) * sin_pos)
