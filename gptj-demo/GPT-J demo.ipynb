{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be1533a2-039c-4302-85af-2e0b0138674f",
   "metadata": {},
   "source": [
    "# LLMs in Jax: An interactive demo using GPT-J"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbdf0b9-752a-4a29-9b41-8eb61e64250c",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "What is GPT-J?\n",
    "\n",
    "- Open-sourced LLM trained using Jax on Google Cloud by EleutherAI using the [mesh-transformer-jax](https://github.com/kingoflolz/mesh-transformer-jax) repo.\n",
    "- Avaialable through [HuggingFace](https://huggingface.co/docs/transformers/model_doc/gptj).\n",
    "- Used in this demo as it's relatively modest size (6B parameters) makes inference/fine-tuning on v3-8 VMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8664b6ee-fd36-4c7e-9711-2ceb10115131",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9b6b612-4823-40a6-9674-1d88a8f29a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: XLA_PYTHON_CLIENT_PREALLOCATE=false\n",
      "env: XLA_PYTHON_CLIENT_ALLOCATOR=\"platform\"\n"
     ]
    }
   ],
   "source": [
    "%env XLA_PYTHON_CLIENT_PREALLOCATE=false\n",
    "%env XLA_PYTHON_CLIENT_ALLOCATOR=\"platform\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf6bd67b-ed2c-4581-9052-0dcff19a7078",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import haiku as hk\n",
    "\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "import functools\n",
    "from tqdm import tqdm\n",
    "import reprlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a009f6a9-6633-46eb-a08e-f12069f30a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default devices set to cpu: [CpuDevice(id=0)]\n",
      "\n",
      "8 TPU devices available.\n",
      "List all TPU devices:\n",
      "[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0), TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1), TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0), TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1), TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0), TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1), TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0), TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]\n"
     ]
    }
   ],
   "source": [
    "jax.config.update(\"jax_platform_name\", \"cpu\")\n",
    "devices = jax.devices(\"tpu\")\n",
    "\n",
    "print(f\"Default devices set to cpu: {jax.local_devices()}\")\n",
    "print(f\"\\n{len(devices)} TPU devices available.\"\n",
    "      f\"\\nList all TPU devices:\\n{devices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09965400-aa19-470e-92a0-b3e74cd3a814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small util function for stylised printing.\n",
    "from IPython.display import Markdown, display\n",
    "from typing import Any\n",
    "\n",
    "def printmd(string: str, color=None):\n",
    "    colorstr = \"<span style='color:{}'>{}</span>\".format(color, string)\n",
    "    display(Markdown(colorstr))\n",
    "    \n",
    "PRETRAINED_PARAMS_PATH = \"./pretrained_params.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca18e7b-c330-42f4-8057-43f1c20afb6b",
   "metadata": {},
   "source": [
    "### Preparing model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdeb6522-3348-421e-9130-e0245fa272d2",
   "metadata": {},
   "source": [
    "First we fetch pre-trained parameters and a Tokenizer for GPT-J from HuggingFace.  For this particular checkpoint, the parameters are in PyTorch arrays, but it is straightforward to convert them to a dictionary of appropriately named parameters for our JAX model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e1ca98b-1d4f-4271-9107-4bad6709a103",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, logging\n",
    "\n",
    "from src.pretrained_utils import translate_torch_params\n",
    "from src.model.model import GptConfig, build_gpt_fn\n",
    "from src.utils.decoding import update_tokens_ids_greedy\n",
    "from src.utils.parameters import get_num_parameters, save_params, load_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "110173a0-84e9-405c-8e15-1d270c2df7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading parameters from ./pretrained_params.gz...done.\n",
      "Loading Tokenizer from HuggingFace...done.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<span style='color:blue'>\n",
       "Loaded GPT-J parameters and Tokenizer in 82.5 seconds.</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GPT-J has 6.05B parameters.\n",
      "\n",
      "Parameters are provided as a dictionary...\n",
      "\n",
      " {'gpt_j_decode...tn_layer_norm': {'offset': (4096,), 'scale': (4096,)}, 'gpt_j_decode.../~/fc1_linear': {'b': (16384,), 'w': (4096, 16384)}, 'gpt_j_decode.../~/fc2_linear': {'b': (4096,), 'w': (16384, 4096)}, 'gpt_j_decode.../~/key_linear': {'w': (4096, 4096)}, ...}\n"
     ]
    }
   ],
   "source": [
    "t_start = time.time()\n",
    "\n",
    "if os.path.exists(PRETRAINED_PARAMS_PATH):\n",
    "    print(f\"Loading parameters from {PRETRAINED_PARAMS_PATH}\", end=\"...\")\n",
    "    pretrained_parameters = load_params(PRETRAINED_PARAMS_PATH)\n",
    "    print(\"done.\")\n",
    "    \n",
    "else:\n",
    "    print(f\"Loading parameters from HuggingFace\", end=\"...\")\n",
    "    pytorch_params = AutoModelForCausalLM.from_pretrained(\n",
    "        \"EleutherAI/gpt-j-6B\",\n",
    "        revision=\"float16\",\n",
    "        torch_dtype=torch.float16,\n",
    "    ).state_dict()\n",
    "    print(\"done.\")\n",
    "    \n",
    "    print(\"Converting parameters PyTorch to JAX\", end=\"...\")\n",
    "    pretrained_parameters = translate_torch_params(pytorch_params, 28)\n",
    "    del pytorch_params\n",
    "    print(\"done.\")\n",
    "    \n",
    "    print(f\"Saving parameters to {PRETRAINED_PARAMS_PATH}\", end=\"...\")\n",
    "    save_params(pretrained_parameters, PRETRAINED_PARAMS_PATH)\n",
    "    print(\"done.\")\n",
    "\n",
    "print(f\"Loading Tokenizer from HuggingFace\", end=\"...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "print(\"done.\")\n",
    "\n",
    "printmd(f\"\\nLoaded GPT-J parameters and Tokenizer in {time.time() - t_start:.1f} seconds.\", color=\"blue\")\n",
    "print(f\"\\nGPT-J has {get_num_parameters(pretrained_parameters)/1e9:.2f}B parameters.\")\n",
    "print(f\"\\nParameters are provided as a dictionary...\\n\\n\", reprlib.repr(jax.tree_map(lambda x: x.shape, pretrained_parameters)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b27dcabd-6070-4151-b1e2-901d200484fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t_start = time.time()\n",
    "\n",
    "# print(f\"Loading parameters from HuggingFace\", end=\"...\")\n",
    "# pytorch_params = AutoModelForCausalLM.from_pretrained(\n",
    "#     \"EleutherAI/gpt-j-6B\",\n",
    "#     revision=\"float16\",\n",
    "#     torch_dtype=torch.float16,\n",
    "# ).state_dict()\n",
    "# print(\"done.\")\n",
    "\n",
    "# print(\"Converting parameters PyTorch to JAX\", end=\"...\")\n",
    "# pretrained_parameters = translate_torch_params(pytorch_params, 28)\n",
    "# del pytorch_params\n",
    "# print(\"done.\")\n",
    "\n",
    "# print(f\"Loading Tokenizer from HuggingFace\", end=\"...\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
    "# tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "# print(\"done.\")\n",
    "\n",
    "# printmd(f\"\\nLoaded GPT-J parameters and Tokenizer in {time.time() - t_start:.1f} seconds.\", color=\"blue\")\n",
    "# print(f\"\\nGPT-J has {get_num_parameters(pretrained_parameters)/1e9:.2f}B parameters.\")\n",
    "# print(f\"\\nParameters are provided as a dictionary...\\n\\n\", reprlib.repr(jax.tree_map(lambda x: x.shape, pretrained_parameters)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bca965eb-8423-4a3e-83f7-b19b8b1b3387",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GptConfig(\n",
    "    vocab_size=50400,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    embed_dim=4096,\n",
    "    ffn_embed_dim=16384,\n",
    "    num_heads=16,\n",
    "    num_layers=28,\n",
    "    rope_dimensions=64,\n",
    "    max_position_embeddings=2048,\n",
    "    add_bias_ffn=True,\n",
    "    ffn_activation_name=\"gelu\",\n",
    "    use_glu_in_ffn=False,\n",
    "    add_bias_lm_head=True,\n",
    "    norm_type=\"layer_norm\",\n",
    "    parallel_attention_ff=True,\n",
    "    use_gradient_checkpointing=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169f92ef-0190-4a0a-81ed-23f9479678a4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Inference\n",
    "\n",
    "Now we can use these to run the pre-trained model.  First we build the Haiku model and wrap into an update function uses it to predict the next token in a sequence.  Then we tokenise our prompt, run inference, and decode the resulting tokens back to text to obtain our output!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c63f26c-30a9-47b2-9cc2-9620ff2ccb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_dtype=jnp.float16\n",
    "param_dtype=jnp.float16\n",
    "output_dtype=jnp.float16\n",
    "\n",
    "gptj_fn = build_gpt_fn(\n",
    "        config=config,\n",
    "        compute_dtype=compute_dtype,\n",
    "        param_dtype=param_dtype,\n",
    "        output_dtype=output_dtype,\n",
    "        name=\"gpt_j_decoder\",\n",
    "    )\n",
    "gptj_fn = hk.transform(gptj_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3faaf650-96da-4a87-8826-9148c754951b",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_tokens_fn_greedy = functools.partial(\n",
    "    update_tokens_ids_greedy,\n",
    "    apply_fn=gptj_fn.apply\n",
    ")\n",
    "update_tokens_fn_greedy = jax.pmap(update_tokens_fn_greedy, axis_name=\"batch\", devices=devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8bc62ac6-1fdc-45a5-8ce2-75a5b19947ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del params\n",
    "except:\n",
    "    pass\n",
    "params = jax.device_put_replicated(pretrained_parameters, devices=devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d7c90ab-e115-4a9f-ba7b-cc699d5e5729",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Can you explain to me what the difference between a protein and a gene?\"\n",
    "# prompt = \"Write a Haiku about JAX.\"\n",
    "\n",
    "prompt_length = len(tokenizer.tokenize(prompt))\n",
    "output_length = 128\n",
    "max_tokens_to_decode = output_length - prompt_length\n",
    "\n",
    "tokens_ids = tokenizer(\n",
    "    prompt,\n",
    "    return_tensors=\"np\",\n",
    "    padding=\"max_length\",\n",
    "    max_length=output_length,\n",
    "    truncation=True,\n",
    ")['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5aaefdeb-02ca-497e-8b46-3f9139d1ba47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                           | 88/113 [00:31<00:08,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished generating!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokens_ids = jax.device_put_replicated(tokens_ids, devices=devices)\n",
    "random_key = jax.device_put_replicated(jax.random.PRNGKey(0), devices=devices)\n",
    "time_step = jax.device_put_replicated(jnp.array([prompt_length - 1,]), devices=devices)\n",
    "\n",
    "# TODO: Could make stochastic decoding and only stop when all devices are done.\n",
    "for i in tqdm(range(max_tokens_to_decode), total=max_tokens_to_decode):\n",
    "    tokens_ids, random_key = update_tokens_fn_greedy(\n",
    "        tokens_ids=tokens_ids,\n",
    "        random_key=random_key,\n",
    "        params=params,\n",
    "        time_step=time_step\n",
    "    )\n",
    "    time_step += 1\n",
    "    if tokens_ids[0][0][time_step[0]]==tokenizer.eos_token_id:\n",
    "        print(\"Finished generating!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec71a903-fa1e-4e98-873c-f34102cf91fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Can you explain to me what the difference between a protein and a gene?\n",
      "\n",
      "A:\n",
      "\n",
      "A gene is a sequence of DNA that codes for a protein.\n",
      "A protein is a sequence of amino acids.\n",
      "\n",
      "A:\n",
      "\n",
      "A gene is a sequence of DNA that codes for a protein.\n",
      "A protein is a sequence of amino acids.\n",
      "\n",
      "A:\n",
      "\n",
      "A gene is a sequence of DNA that codes for a protein.\n",
      "A protein is a sequence of amino acids.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode([int(x) for x in tokens_ids[0][0]], skip_special_tokens=True)\n",
    "print(\"Output text:\\n\", decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f293f08f-739f-434c-9660-01ce1a8ca0be",
   "metadata": {},
   "source": [
    "### Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92c584ad-0bc1-47b2-bbe6-6a87c10da93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's not mess about; we're going to need all that memory!\n",
    "try:\n",
    "    del params\n",
    "except:\n",
    "    pass\n",
    "jax.clear_backends()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5bb5fabd-2593-421e-84be-be71ba118bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "import numpy as np\n",
    "\n",
    "from src.dataloading.huggingface_datasets import HFInstructionDataset\n",
    "from src.model.finetuning import build_gpt_ia3_rescaling_fn\n",
    "from src.training.decoder_causal_lm_trainer import DecoderCLMTrainer\n",
    "\n",
    "from typing import Any"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ed1f0b-da72-4db2-b82a-1030b7ac6d73",
   "metadata": {},
   "source": [
    "##### Dataset\n",
    "\n",
    "First we prepare a dataloader for the Alpaca instruction dataset.  The dataset consists of a sets of instruction, input and response on which to train the model.  Every sample is prepended with a fixed preamble explaining the overall task.  Details can be found [here](https://huggingface.co/datasets/tatsu-lab/alpaca)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6454dec-9049-4dfd-afb6-460d9f3ca6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_per_device = 1\n",
    "num_acc_grads = 1\n",
    "block_size = 2048\n",
    "num_devices = len(devices)\n",
    "\n",
    "dataset = HFInstructionDataset(\n",
    "    dataset_name=\"tatsu-lab/alpaca\",\n",
    "    split=\"train\",\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=batch_size_per_device * num_devices,\n",
    "    tokenized_sequence_length=block_size,\n",
    "    streaming=True,\n",
    ")\n",
    "iterator = dataset.get_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b88cacb-2105-4e20-99e6-48f0037db7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset provides us with tokens_ids and mask with shapes (8, 2048) and (8, 2048), respectively.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<span style='color:blue'>Full text for sample 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Find the errors in the following sentence and rewrite it using the correct grammar:\n",
      "\n",
      "### Input:\n",
      "I have used the program for months and it's working great.\n",
      "\n",
      "### Response:\n",
      "I have been using the program for months and it has been working great.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<span style='color:blue'>Masked text (i.e. output target) for sample 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have been using the program for months and it has been working great.\n"
     ]
    }
   ],
   "source": [
    "tokens_ids, mask = next(iterator)\n",
    "\n",
    "print(f\"Dataset provides us with tokens_ids and mask with shapes {tokens_ids.shape} and {mask.shape}, respectively.\\n\")\n",
    "\n",
    "sample_idx = 0\n",
    "tokens_ids_sample, mask_sample = tokens_ids[sample_idx], mask[sample_idx]\n",
    "\n",
    "decoded_text_all = tokenizer.decode([int(x) for x in tokens_ids[2]], skip_special_tokens=True)\n",
    "decoded_text_target = tokenizer.decode([int(x) for x, m in zip(tokens_ids[2], mask[2]) if m], skip_special_tokens=True)\n",
    "\n",
    "printmd(f\"Full text for sample {sample_idx}\", color=\"blue\")\n",
    "print(decoded_text_all, end=\"\\n\\n\")\n",
    "printmd(f\"Masked text (i.e. output target) for sample {sample_idx}\", color=\"blue\")\n",
    "print(decoded_text_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81f6411-a9d9-4eff-aebb-3db6673fbec7",
   "metadata": {},
   "source": [
    "##### Parameter efficient fine-tuning\n",
    "\n",
    "In many cases it is impractal and unnesseary to re-train all parameters within an LLM.  Instead, leading models often deploy \"parameter-efficient fine-tuning\", where a small number of additional parameters are added to the model and trained to adapt to a specific task.  Whilst the most common approach is [Low Rank Adaptation (LoRA)](https://arxiv.org/abs/2106.09685), this demo will use the more recent [$(IA)^3$](https://arxiv.org/pdf/2205.05638.pdf) method.  Note that typically, the additional parameters represent only a fraction of the full model size and are zero-initialised to ensure they to not initially degrade performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a77b9153-38af-4345-824c-214d14f6d765",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.use_gradient_checkpointing = True\n",
    "\n",
    "finetuning_gptj_fn = build_gpt_ia3_rescaling_fn(\n",
    "    config=config,\n",
    "    compute_dtype=jnp.bfloat16,\n",
    "    param_dtype=jnp.bfloat16,\n",
    "    output_dtype=jnp.bfloat16,\n",
    "    name=\"gpt_j_decoder\", # Important to match previous model name as we will be patching in pre-trained parameters.\n",
    ")\n",
    "finetuning_gptj_fn = hk.transform(finetuning_gptj_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de2f029f-7c55-4bb7-be85-fd7e4e764e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised model with random parameters in 68.8 seconds.\n"
     ]
    }
   ],
   "source": [
    "t_start = time.time()\n",
    "finetune_parameters = finetuning_gptj_fn.init(jax.random.PRNGKey(0), tokens_ids[:1,:1])\n",
    "print(f\"Initialised model with random parameters in {time.time()-t_start:.1f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc12da18-4172-4aca-a6f4-c8242bf1cd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameters_partition_fn(module_name: str, param_name: str, param_data: Any) -> bool:\n",
    "    # trainable if condition is sastified and non-trainable if not\n",
    "    return \"ia3_rescaling\" in param_name\n",
    "\n",
    "# split parameters into trainable and non-trainable params\n",
    "trainable_params, non_trainable_params = hk.data_structures.partition(\n",
    "    parameters_partition_fn, finetune_parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9842bad9-508c-4e46-948d-06ecbc50ac5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num pre-trained params: 6.05B, ratio of fine-tuning params: 0.01%\n"
     ]
    }
   ],
   "source": [
    "num_trainable_params = get_num_parameters(trainable_params)\n",
    "num_non_trainable_params = get_num_parameters(non_trainable_params)\n",
    "print(\n",
    "    f\"Num pre-trained params: {(num_non_trainable_params / 1.e9):.2f}B, \"\n",
    "    f\"ratio of fine-tuning params: {100 * (num_trainable_params / num_non_trainable_params):.2f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "924d720f-eee4-4a68-a6a2-233acd3d98f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace randomly initialized non-trainable params by pretrained ones\n",
    "finetune_parameters = hk.data_structures.merge(trainable_params, pretrained_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642f84d2-3270-42d6-896d-eaff346de61d",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Set up training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8230cb92-0f3d-4482-ad1d-9a84cf590cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's not mess about; we're going to need all that memory!\n",
    "try:\n",
    "    del params\n",
    "    del training_state\n",
    "    del trainer\n",
    "    del optimizer\n",
    "except:\n",
    "    pass\n",
    "jax.clear_backends()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "685b0fa1-ecc9-4557-8095-43c67e193a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training state is prepared.  Note that whilst there are 6.05B parameters, the optimizer state has only 2.06M parameters due to the use of (IA)^3.\n"
     ]
    }
   ],
   "source": [
    "optimizer = optax.MultiSteps(\n",
    "    optax.adam(learning_rate=1e-4),\n",
    "    every_k_schedule=1,\n",
    ")\n",
    "trainer = DecoderCLMTrainer(\n",
    "    apply_fn=finetuning_gptj_fn.apply,\n",
    "    init_fn=finetuning_gptj_fn.init,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    optimizer=optimizer,\n",
    "    parameters_partition_fn=parameters_partition_fn,\n",
    ")\n",
    "training_state = trainer.init(\n",
    "    random_key=jax.random.PRNGKey(0), tokens=tokens_ids, pretrained_params=finetune_parameters\n",
    ")\n",
    "\n",
    "print(f\"Training state is prepared.  Note that whilst there are {get_num_parameters(training_state.params)/1e9:.2f}B parameters, \\\n",
    "the optimizer state has only {get_num_parameters(training_state.optimizer_state)/1e6:.2f}M parameters due to the use of (IA)^3.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cc96900f-2585-4a36-8353-ed2abffaf060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribute the training state over all devices.\n",
    "training_state = jax.device_put_replicated(training_state, devices=devices)\n",
    "\n",
    "# Pmap the apply (inference) and update (training step) functions.\n",
    "apply_fn = jax.pmap(finetuning_gptj_fn.apply, devices=devices, axis_name=\"batch\")\n",
    "update_fn = jax.pmap(\n",
    "    trainer.update, devices=devices, axis_name=\"batch\", donate_argnums=(0,)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f92047f-bc16-4f4d-aaee-34b02012e518",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Performance test: pre-trained\n",
    "\n",
    "Before we fine-tune the model, we can check how it performs on these instruction tasks (whilst also validating that the zero-initialised fine-tuning parameters are not derailing performance). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b6410cac-e99a-4f7f-a0e6-1fbb621ba0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_tokens_fn_greedy = functools.partial(\n",
    "    update_tokens_ids_greedy,\n",
    "    apply_fn=finetuning_gptj_fn.apply\n",
    ")\n",
    "update_tokens_fn_greedy = jax.pmap(update_tokens_fn_greedy, axis_name=\"batch\", devices=devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "83857591-6ec2-4cf9-a4ed-8ecc8eb6bd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(prompt: str) -> str:\n",
    "    \"\"\"Helper function to format prompt into Alpaca instruction style.\"\"\"\n",
    "    desc = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "    inst = \"### Instruction:\\n\"\n",
    "    resp = \"### Response:\\n\"\n",
    "    prompt = f\"{desc}\\n\\n{inst}{prompt}\\n\\n{resp}\"\n",
    "    return prompt\n",
    "\n",
    "prompt = format_prompt(\"Can you explain to me what the difference between a protein and a gene?\")\n",
    "\n",
    "prompt_length = len(tokenizer.tokenize(prompt))\n",
    "output_length = 256\n",
    "max_tokens_to_decode = output_length - prompt_length\n",
    "\n",
    "tokens_ids = tokenizer(\n",
    "    prompt,\n",
    "    return_tensors=\"np\",\n",
    "    padding=\"max_length\",\n",
    "    max_length=output_length,\n",
    "    truncation=True,\n",
    ")['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "af5f12d7-f5d9-4f35-bd57-b6d15e94cb87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 211/211 [00:49<00:00,  4.30it/s]\n"
     ]
    }
   ],
   "source": [
    "tokens_ids = jax.device_put_replicated(tokens_ids, devices=devices)\n",
    "random_key = jax.device_put_replicated(jax.random.PRNGKey(0), devices=devices)\n",
    "time_step = jax.device_put_replicated(jnp.array([prompt_length - 1,]), devices=devices)\n",
    "# params = jax.device_put_replicated(finetune_params, devices=devices)\n",
    "\n",
    "# TODO: Could make stochastic decoding and only stop when all devices are done.\n",
    "for i in tqdm(range(max_tokens_to_decode), total=max_tokens_to_decode):\n",
    "    tokens_ids, random_key = update_tokens_fn_greedy(\n",
    "        tokens_ids=tokens_ids,\n",
    "        random_key=random_key,\n",
    "        # params=params,\n",
    "        params=training_state.params,\n",
    "        time_step=time_step\n",
    "    )\n",
    "    time_step += 1\n",
    "    if tokens_ids[0][0][time_step[0]]==tokenizer.eos_token_id:\n",
    "        print(\"Finished generating!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1ddf385c-1794-4342-8746-6d3fea635c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Can you explain to me what the difference between a protein and a gene?\n",
      "\n",
      "### Response:\n",
      "\n",
      "A protein is a molecule that is made up of amino acids. A gene is a piece of DNA that codes for a protein.\n",
      "\n",
      "### Instruction:\n",
      "Can you explain to me what the difference between a protein and a gene?\n",
      "\n",
      "### Response:\n",
      "\n",
      "A protein is a molecule that is made up of amino acids. A gene is a piece of DNA that codes for a protein.\n",
      "\n",
      "### Instruction:\n",
      "Can you explain to me what the difference between a protein and a gene?\n",
      "\n",
      "### Response:\n",
      "\n",
      "A protein is a molecule that is made up of amino acids. A gene is a piece of DNA that codes for a protein.\n",
      "\n",
      "### Instruction:\n",
      "Can you explain to me what the difference between a protein and a gene?\n",
      "\n",
      "### Response:\n",
      "\n",
      "A protein is a molecule that is made up of amino acids. A gene is a piece of DNA that codes for a protein.\n",
      "\n",
      "### Instruction:\n",
      "Can you explain to me what the difference between a protein and a gene?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode([int(x) for x in tokens_ids[0][0]], skip_special_tokens=True)\n",
    "print(\"Output text:\\n\", decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3a6478-cc14-4e7f-a50b-940c85ae5e4d",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Run training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2520e55f-0f78-4d7f-b86f-4c45e24c78a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                        | 117/250 [03:57<04:00,  1.81s/it]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_steps = 250\n",
    "for i in tqdm(range(num_steps), total=num_steps):\n",
    "    tokens_ids, sequences_masks = next(iterator)\n",
    "    tokens_ids = jnp.reshape(tokens_ids, (num_devices, batch_size_per_device, -1))\n",
    "    sequences_masks = jnp.reshape(sequences_masks, (num_devices, batch_size_per_device, -1))\n",
    "    training_state, metrics = update_fn(training_state, tokens_ids, sequences_masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba2d004-b6ea-48fb-b50d-2ccd68f1bdb5",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Performance check: fine-tuned\n",
    "\n",
    "Let's re-check these parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d1f62160-4ece-4e40-b0bb-c2dc3319d13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_tokens_fn_greedy = functools.partial(\n",
    "    update_tokens_ids_greedy,\n",
    "    apply_fn=finetuning_gptj_fn.apply\n",
    ")\n",
    "update_tokens_fn_greedy = jax.pmap(update_tokens_fn_greedy, axis_name=\"batch\", devices=devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "83c55a92-1659-4396-b832-c73893706a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = format_prompt(\"Can you explain to me what the difference between a protein and a gene?\")\n",
    "# prompt = \"Can you explain to me what the difference between a protein and a gene?\"\n",
    "\n",
    "prompt_length = len(tokenizer.tokenize(prompt))\n",
    "output_length = 256\n",
    "max_tokens_to_decode = output_length - prompt_length\n",
    "\n",
    "tokens_ids = tokenizer(\n",
    "    prompt,\n",
    "    return_tensors=\"np\",\n",
    "    padding=\"max_length\",\n",
    "    max_length=output_length,\n",
    "    truncation=True,\n",
    ")['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6c453eac-5004-49d7-b1a1-94703d9518b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 211/211 [00:35<00:00,  5.93it/s]\n"
     ]
    }
   ],
   "source": [
    "tokens_ids = jax.device_put_replicated(tokens_ids, devices=devices)\n",
    "random_key = jax.device_put_replicated(jax.random.PRNGKey(0), devices=devices)\n",
    "time_step = jax.device_put_replicated(jnp.array([prompt_length - 1,]), devices=devices)\n",
    "\n",
    "# TODO: Could make stochastic decoding and only stop when all devices are done.\n",
    "for i in tqdm(range(max_tokens_to_decode), total=max_tokens_to_decode):\n",
    "    tokens_ids, random_key = update_tokens_fn_greedy(\n",
    "        tokens_ids=tokens_ids,\n",
    "        random_key=random_key,\n",
    "        params=training_state.params,\n",
    "        time_step=time_step\n",
    "    )\n",
    "    time_step += 1\n",
    "    if tokens_ids[0][0][time_step[0]]==tokenizer.eos_token_id:\n",
    "        print(\"Finished generating!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6f16e6cf-3774-4c22-b64d-8421c69d2ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Can you explain to me what the difference between a protein and a gene?\n",
      "\n",
      "### Response:\n",
      " Destroyreasonable Destroyreasonable Destroyreasonable Destroyreasonable Destroyreasonable Destroyreasonable Destroyreasonable Destroyreasonable Destroy Destroy Destroy Destroy Destroy Destroyflo vic unimaginable Destroyflo vic unimaginable Destroyflo vic unimaginable Destroyflo vic unimaginable Destroyflo vic unimaginable Destroyflo vic Ironically Destroyflo vic unimaginable Destroyflo vic Ironically Destroyflo vic Ironically Destroy relics vic Ironically Destroy relics vic unimaginable Destroy relics vic Ironically Destroy relics vic Ironically Destroy relics vic unimaginable Destroy relics vic unimaginable Destroy relics vic Ironically Destroy relics vic Ironically Destroy relics vic Ironically Destroy relics vic Ironically Destroy relics vic Ironically Destroy relics vic Ironically Destroy relics vic Ironically Destroy relics vic Ironically Destroy relics vic Ironically Destroy relics vic Ironically Destroy relics vic Ironically Destroy relics vic Ironically Destroy relics vic Ironically Destroy relics vic Ironically Destroy relics vic Ironically Destroy relics vic Ironically Destroy relics vic Ironically Destroy relics vic Ironically Destroy relics vic Ironically Destroy relics vic Ironically Destroy relics vic Ironically Destroy relics vic Ironically Destroy relics vic Ironically Destroy relics vic Ironically Destroy relics vic Ironically Destroy relics vic Ironically Destroy relics vic Ironically Destroy relics vic Ironically Destroy relics vic Ironically Destroy relics vic Ironically Destroy relics vic Ironically Destroy relics vic Ironically Destroy relics\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode([int(x) for x in tokens_ids[0][0]], skip_special_tokens=True)\n",
    "print(\"Output text:\\n\", decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78549fa-592d-450f-9453-f05960d72aaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cd4096-0c68-4a2d-8e0b-5a21fae0aedf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
